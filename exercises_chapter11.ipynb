{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design & Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises in Algorithm Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Issues to Consider\n",
    "- How many queries are required, and what kind of composition can we use?\n",
    "  - Is parallel composition possible?\n",
    "  - Should we use sequential composition, advanced composition, or a variant of differential privacy?\n",
    "- Can we use the sparse vector technique?\n",
    "- Can we use the exponential mechanism?\n",
    "- How should we distribute the privacy budget?\n",
    "- If there are unbounded sensitivities, how can we bound them?\n",
    "- Would synthetic data help?\n",
    "- Would post-processing to \"de-noise\" help?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generalized Sample and Aggregate\n",
    "\n",
    "Design a variant of sample and aggregate which does *not* require the analyst to specify the output range of the query function $f$.\n",
    "\n",
    "```{tip}\n",
    "Use SVT to find good upper and lower bounds on $f(x)$ for the whole dataset first. The result of $clip(f(x), lower, upper)$ has bounded sensitivity, so we can use this query with SVT. Then use sample and aggregate with these upper and lower bounds.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T15:00:46.268024Z",
     "start_time": "2025-11-23T15:00:44.214222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "adult = pd.read_csv(\"adult_with_pii.csv\")\n",
    "\n",
    "#svt implementation\n",
    "def above_threshold(queries, df, T, epsilon,on_fail=\"random\"):\n",
    "    T_hat = T + np.random.laplace(loc=0, scale = 2/epsilon)\n",
    "    for idx, q in enumerate(queries):\n",
    "        nu_i = np.random.laplace(loc=0, scale = 4/epsilon)\n",
    "        if q(df) + nu_i >= T_hat:\n",
    "            return idx\n",
    "\n",
    "    # Handle failure case\n",
    "    if on_fail == \"random\":\n",
    "        print(\"random int returned\")\n",
    "        # if the algorithm \"fails\", return a random index\n",
    "        # more convenient in certain use cases\n",
    "        return random.randint(0, len(queries) - 1)\n",
    "    else:\n",
    "        return None\n",
    "#our query\n",
    "def age_sum_query(df):\n",
    "    return df['Age'].sum()\n",
    "# code from the book to compute the upper bound, and their respective counterparts for the lower bound\n",
    "def create_query(query, b):\n",
    "    return lambda df: age_sum_query(df) - b\n",
    "\n",
    "def create_query_lower(query, b):\n",
    "    return lambda df: b - age_sum_query(df)\n",
    "\n",
    "bs = range(1750000,900000,-14)\n",
    "bs_lower = range(900000,1750000, 14)\n",
    "queries = [create_query(age_sum_query(adult),b) for b in bs]\n",
    "queries_lower = [create_query_lower(age_sum_query(adult),b) for b in bs_lower]\n",
    "epsilon = .1\n",
    "\n",
    "upper_sum = bs[above_threshold(queries, adult, 0, epsilon)]\n",
    "lower_sum = bs_lower[above_threshold(queries_lower, adult, 0, epsilon)]\n",
    "print(\"lower bound: \", lower_sum , \"\\nupper bound: \", upper_sum , \"\\nactual value: \" , age_sum_query(adult))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower bound:  1360166 \n",
      "upper bound:  1360324 \n",
      "actual value:  1360238\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T14:36:07.757285Z",
     "start_time": "2025-11-23T14:36:07.739792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity/epsilon)\n",
    "\n",
    "# saa_age_sum is based on saa_avg_age from chapter 7\n",
    "def saa_age_sum(k, epsilon, logging=False):\n",
    "    df = adult\n",
    "\n",
    "    # Calculate the number of rows in each chunk\n",
    "    chunk_size = int(np.ceil(df.shape[0] / k))\n",
    "\n",
    "    if logging:\n",
    "        print(f'Chunk size: {chunk_size}')\n",
    "\n",
    "    # Step 1: split `df` into chunks\n",
    "    xs      = [df.iloc[i:i+chunk_size] for i in range(0,df.shape[0],chunk_size)]\n",
    "\n",
    "    # Step 2: run f on each x_i and clip its output\n",
    "    answers = [age_sum_query(x_i) for x_i in xs]\n",
    "\n",
    "    u = upper_sum / k\n",
    "    l = lower_sum / k\n",
    "    clipped_answers = np.clip(answers, l, u)\n",
    "\n",
    "    # Step 3: take the noisy sum of the clipped answers\n",
    "    noisy_sum = laplace_mech(np.sum(clipped_answers), (u-l), epsilon)\n",
    "    return noisy_sum\n",
    "\n",
    "saa_age_sum(600, 1, logging=True)\n",
    "#epsilon = 1,2 (0,1 x 2 for upper_sum and lower_sum berechnung + 1 for saa)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1344329.2259279098)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics\n",
    "\n",
    "Design an algorithm to produce differentially private versions of the following statistics:\n",
    "\n",
    "- Mean: $\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i$\n",
    "- Variance: $var = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2$\n",
    "- Standard deviation: $\\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2}$\n",
    "\n",
    "**Ideas**:\n",
    "\n",
    "**Mean**\n",
    "\n",
    "1. Use SVT to find upper and lower clipping bounds\n",
    "2. Compute noisy sum and count, and derive mean by post-processing\n",
    "\n",
    "**Variance**\n",
    "\n",
    "1. Split it into a count query ($\\frac{1}{n}$ - we have the answer from above) and a sum query\n",
    "2. What's the sensitivity of $\\sum_{i=1}^n (x_i - \\mu)^2$? It's $b^2$; we can clip and compute $\\sum_{i=1}^n (x_i - \\mu)^2$, then multiply by (1) by post processing\n",
    "\n",
    "**Standard Deviation**\n",
    "\n",
    "1. Just take the square root of variance\n",
    "\n",
    "Total queries:\n",
    "- Lower clipping bound (SVT)\n",
    "- Upper clipping bound (SVT)\n",
    "- Noisy sum (mean)\n",
    "- Noisy count\n",
    "- Noisy sum (variance)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T15:05:34.599940Z",
     "start_time": "2025-11-23T15:05:34.552772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#mean\n",
    "#compute upper bound via SVT\n",
    "\n",
    "def age_sum_query(df, b):\n",
    "    return df['Age'].clip(lower=0, upper=b).sum()\n",
    "\n",
    "def create_query(b):\n",
    "    return lambda df: age_sum_query(df, b) - age_sum_query(df, b + 1)\n",
    "\n",
    "bs = range(1,150,2)\n",
    "queries = [create_query(b) for b in bs]\n",
    "\n",
    "#compute lower bound via SVT\n",
    "\n",
    "def age_sum_query_lower(df, b):\n",
    "    return df['Age'].clip(lower=b, upper=150).sum()\n",
    "\n",
    "def create_query_lower(b):\n",
    "    return lambda df: age_sum_query_lower(df, b) - age_sum_query_lower(df, b - 1)\n",
    "\n",
    "bs_lower = range(150,1,-2)\n",
    "queries_lower = [create_query_lower(b) for b in bs]\n",
    "epsilon = .1\n",
    "upper_age = bs[above_threshold(queries, adult, 0, epsilon)]\n",
    "lower_age = bs[above_threshold(queries_lower, adult, 0, epsilon)]\n",
    "print(\"lower bound: \", lower_age, \" upper bound: \", upper_age)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower bound:  3  upper bound:  95\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T15:05:44.770431Z",
     "start_time": "2025-11-23T15:05:44.767055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dp_mean_variance_stddef(epsilon):\n",
    "    df = adult[\"Age\"]\n",
    "\n",
    "    answers = df.values\n",
    "\n",
    "    u = upper_age\n",
    "    l = lower_age\n",
    "    clipped_answers = np.clip(answers, l, u)\n",
    "\n",
    "    noisy_sum = laplace_mech(clipped_answers.sum(), (u-l), epsilon)\n",
    "    noisy_count = laplace_mech(df.shape[0], 1, epsilon)\n",
    "    one_div_n = 1 / noisy_count #needed later\n",
    "    noisy_mean = one_div_n * noisy_sum\n",
    "\n",
    "    var_sensitivity = max((l-noisy_mean)**2,(u-noisy_mean)**2)\n",
    "    squared_diff = (clipped_answers - noisy_mean) ** 2\n",
    "    noisy_sum_variance = laplace_mech(squared_diff.sum(), var_sensitivity, epsilon)\n",
    "    noisy_variance = one_div_n * noisy_sum_variance\n",
    "    noisy_stddev = np.sqrt(noisy_variance)\n",
    "\n",
    "    return noisy_mean, noisy_variance, noisy_stddev\n",
    "\n",
    "mean, variance, stddev = dp_mean_variance_stddef(1)\n",
    "print(\"mean: \", mean, \"\\nvariance: \", variance, \"\\nstandard deviation: \", stddev)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  41.77131595929292 \n",
      "variance:  320.78817140332126 \n",
      "standard deviation:  17.910560331919303\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Heavy Hitters\n",
    "\n",
    "Google's RAPPOR system {cite}`rappor` is designed to find the most popular settings for Chrome's home page. Design an algorithm which:\n",
    "\n",
    "- Given a list of the 10,000 most popular web pages by traffic,\n",
    "- Determines the top 10 most-popular home pages out of the 10,000 most popular web pages\n",
    "\n",
    "\n",
    "```{tip}\n",
    "Use parallel composition and take the noisy top 10\n",
    "```\n",
    "\n",
    "```{note}\n",
    "RAPPOR operates in the **local** model of differential privacy, which will be introduced later on. Feel free to skip this exercise for now and return later!\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Lösung**\n",
    "\n",
    "Ich interpretiere, dass ich Zugang zu den Nutzern habe und abfragen kann, ob Website x Ihre Homepage ist.\n",
    "\n",
    "Ich interpretiere auch, dass diese Info locally dp zurückkommt.\n",
    "\n",
    "Laut ChatGPT verwendet RAPPOR einen Mechanismus pro User Report (essentiell für die privacy Cost weiter unten)\n",
    "\n",
    "10k mal Fragen, ob die Website die Homepage ist, würde 10000 * epsilon kosten\n",
    "\n",
    "Also erstelle ich einen 10000-bit langen Bitstring, die Nutzer sollen den Bit auf 1 setzen, der ihre Homepage ist (kann auch keine der 10k sein)\n",
    "\n",
    "Privacy cost = epsilon, da ein mal mit dem Nutzer interagiert wurde.\n",
    "\n",
    "Die einzelnen Reports sind dank LDP schon noisy , also summiere ich die 1er Bits pro Website und gebe die 10 größten Summen zurück\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Queries\n",
    "\n",
    "Design an algorithm to produce summary statistics for the U.S. Census. Your algorithm should produce total population counts at the following levels:\n",
    "\n",
    "- Census tract\n",
    "- City / town\n",
    "- ZIP Code\n",
    "- County\n",
    "- State\n",
    "- USA\n",
    "\n",
    "```{tip}\n",
    "\n",
    "Idea 1: *Only* compute the bottom level (census tract), using parallel composition. Add up all the tract counts to get the city counts, and so on up the hierarchy. Advantage: lowers privacy budget.\n",
    "\n",
    "Idea 2: Compute counts at all levels, using parallel composition for each level. Tune the budget split using real data; probably we need more accuracy for the smaller levels of the hierarchy.\n",
    "\n",
    "Idea 3: As (2), but also use post-processing to re-scale lower levels of the hierarchy based on higher ones; truncate counts to whole numbers; move negative counts to 0.\n",
    "\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Lösung**\n",
    "\n",
    "Idee 1: Die parallel composition wird hier angewendet, in dem man ein counting query parallel auf alle möglichen census tracts laufen lässt -> privacy cost = epsilon\n",
    "\n",
    "Die Sensitivität von counting queries ist 1, also ist der noisy count gleich q(D) + Laplace(1/epsilon)\n",
    "\n",
    "Die oberen Ebenen sind dann trivial, einfach die entsprechenden noisy counts zusammenfügen. - keine Erhöhung der privacy cost, da das unter post processing fällt."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Idee 2: Wir behalten die counting querys und die Berechnung des Noisy count bei, diesmal aber auf jeder Ebene.\n",
    "\n",
    "Die Aufgabe ändert sich insofern, dass wir hier auch mit sequential composition arbeiten, da jeder Datensatz einmal pro Ebene vorkommt. Dieses Gesamt-Epsilon müssen wir sinnvoll aufteilen, die unteren Ebenen brauchen mehr privacy.\n",
    "\n",
    "Mein Ansatz für die Aufteilung von epsilon wäre wiefolgt. Wir finden heraus, wieviele Möglichkeiten es pro Ebene (1 USA, 50 States, usw). Wir summieren diese Möglichkeiten (M) und rechnen dann mit M(Ebene) / M(Gesamt) den Anteil der Ebene an epsilon_gesamt aus.\n",
    "\n",
    " Da ich laut Angabe 'real data' verwenden soll, hier die Counts und die dazugehörigen Anteile an epsilon_gesamt für die USA:\n",
    "\n",
    "| Ebene  | M  | %  |\n",
    "|---|---|---|\n",
    "| USA  | 1  | 0,00067  |\n",
    "| States  | 50  | 0,03336  |\n",
    "| Countys  | 3244  | 2,16454  |\n",
    "| ZIP Codes  | 41552  | 27,72536  |\n",
    "| Cities / Towns  | 19495  | 13,00794  |\n",
    "| Census tracts  | 85528  | 57,06813  |\n",
    "| Total  | 149870  | 100  |\n",
    "\n",
    "Diese Aufteilung ist heuristisch und rechnerisch leicht nachvollziehbar. Dennoch würde ich noch die Prozent von ZIP Codes und Cities tauschen, da es ja nicht direkt darum geht, wie viele mögliche Werte eine Ebene hat, sondern wie fein die Ebene ist.\n",
    "Je größer epsilon, desto größeren Einfluss hat das Rauschen (hier durch Laplace) auf den Count. Das ist bei der Ebene USA nicht schlimm, aber bei ZIP-Codes zb. schon. Es gibt zwar weniger Städte als Postleitzahlen, aber Städte sind dennoch die feinere Schicht."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Idee 3: Erweiterung von Idee 2. Alles passiert im post processing, der privacy cost erhöht sich also nicht. Wir skalieren die Daten, da die Summen der Ebenen nicht übereinstimmen.\n",
    "Wir berechnen Count(USA) / Summe(Counts(States)) um den Skalierungsparameter herauszufinden.\n",
    "Wir rechnen Skalierungsparameter * Count(State) für alle States. Nun gilt Summe(Counts(States)) = Count(USA).\n",
    "Nun ist States die obere Schicht und Countys die untere. So gehen wir vor, bis wir die unterste Schicht erreicht haben und alle Schichten denselben Count ergeben.\n",
    "Das entfernen von negativen Einträgen sowie das Runden geschieht zum Schluss, da wir es durch die Skalierung sonst mehrmals anwenden müssten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Workloads of Range Queries\n",
    "\n",
    "Design an algorithm to accurately answer a workload of *range queries*. Range queries are queries on a single table of the form: \"how many rows have a value that is between $a$ and $b$?\" (i.e. the count of rows which lie in a specific range). \n",
    "\n",
    "### Part 1\n",
    "The whole workload is pre-specified as a finite sequence of ranges: $\\{(a_1, b_1), \\dots, (a_k, b_k)\\}$, and \n",
    "\n",
    "### Part 2\n",
    "The length of the workload $k$ is pre-specified, but queries arrive in a streaming fashion and must be answered as they arrive.\n",
    "\n",
    "### Part 3\n",
    "The workload may be infinite.\n",
    "\n",
    "**Ideas**:\n",
    "\n",
    "Just run each query with sequential composition.\n",
    "\n",
    "For part 1, combine them so we can use $L2$ sensitivity. When $k$ is large, this will work well with Gaussian noise.\n",
    "\n",
    "Or, build synthetic data:\n",
    "\n",
    "- For each range $(i, i+1)$, find a count (parallel composition). This is a synthetic data representation! We can answer infinitely many queries by adding up the counts of all the segments in this histogram which are contained in the desired interval.\n",
    "- For part 2, use SVT\n",
    "\n",
    "For SVT: for each query in the stream, ask how far the real answer is from the synthetic data answer. If it's far, query the real answer's range (as a histogram, using parallel composition) and update the synthetic data. Otherwise just give the synthetic data answer. This way you *ONLY* pay for updates to the synthetic data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Lösung**\n",
    "\n",
    "Jedes Query hintereinander zu beantworten wäre die naive Implementation, aber mit hoher privacy cost von k * epsilon (gilt für parts 1+2, bei Part 3 wäre die privacy cost im worst case unendlich)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Part 1: synthetic Data - wir erstellen ein Histogramm mit einer Bin pro Wert, für jede Bin wird ein Count-Query ausgeführt. durch parallel computing ist der privacy cost epsilon.\n",
    "\n",
    "Der Range Count ist dann die Summe der noisy counts der Bins, die in der entsprechenden Range liegen.\n",
    "\n",
    "Part 2: Wir bleiben beim Histogramm. Wir legen zusätzlich einen Threshold fest, unter dem der Fehler (mit Noise) bleiben muss, damit die synthetische Antwort ausgegeben werden kann.\n",
    "\n",
    "Der Fehler ist `| RangeQuery(a,b) - SummeBins(a,b) |`. Um DP-konform zu bleiben verwendet SVT den verrauschten Fehler noisyError = `| RangeQuery(a,b) + Rauschen) - SummeBins(a,b) |`.\n",
    "\n",
    "Wenn nun die Ungleichung noisyError >= (Threshold + Rauschen) wahr ist, wird eine Range Query auf die echten Daten ausgeführt. Im Zuge dessen werden auch die betroffenen Histogramm-Bins den echten Daten angepasst.\n",
    "\n",
    "Somit entstehen extra privacy costs nur wenn die synthetischen Daten zu weit von dem eigentlichen Wert entfernt sind. Und da die synthetischen Daten danach angepasst werden, passiert das maximal einmal pro Query. Das Abfragen des RangeQuery bei der Berechnung des Fehlers fordert keine extra privacy cost, da das Ergebnis intern bleibt.\n",
    "\n",
    "Part 3: Hier können wir die Strategie von Part 2 übernehmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist for Privacy Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment of differential privacy involves several steps to ensure the protection of sensitive data while still allowing useful analysis. Here's a checklist for deploying a system with differential privacy:\n",
    "\n",
    "1. **Data Classification**: Identify and classify sensitive data that needs protection. Understand the sensitivity levels and potential risks associated with each class of data. This should include determination of the [Unit of Privacy](https://programming-dp.com/ch3.html#the-unit-of-privacy) as we previously discussed.\n",
    "\n",
    "2. **Data Preparation**: Preprocess the data to remove personally identifiable information (PII) and other sensitive attributes that are not relevant to the analysis.\n",
    "\n",
    "3. **Define Privacy Budget**: Determine acceptable levels of utility or privacy risk for your deployment. This will govern the amount of noise added to the data to achieve differential privacy.\n",
    "\n",
    "4. **Data Analysis Requirements**: Clearly outline the data analysis goals, as well as specific transformations and queries that need to be supported while maintaining privacy. Ensure that the chosen privacy mechanisms and privacy unit can accommodate these requirements.\n",
    "\n",
    "5. **Implement Differential Privacy**: Choose appropriate differential privacy mechanisms such as Laplace mechanism, Gaussian mechanism, or other advanced techniques based on the analysis requirements. Implement these mechanisms into the data processing pipeline. Identify a strategy for privacy composition if applicable.\n",
    "\n",
    "6. **Noise Generation**: Generate and introduce high-quality entropy to the data in accordance with the chosen differential privacy mechanism. Ensure that the randomness level is calibrated to achieve the desired privacy guarantee.\n",
    "\n",
    "7. **Testing and Verification**: Conduct thorough testing to validate the effectiveness of the deployed differential privacy mechanisms. Test the system with a variety of queries and scenarios to ensure that privacy is preserved while maintaining data utility. Ideally, conduct tests on public or synthetic data.\n",
    "\n",
    "8. **Performance Evaluation**: Evaluate the performance overhead introduced by the time and storage cost of differential privacy mechanisms. Monitor system metrics such as latency, throughput, and compute resource utilization to ensure acceptable levels of efficiency.\n",
    "\n",
    "9. **Documentation and Compliance**: Document the deployment process, including the differential privacy mechanisms used, privacy parameters chosen, and any other relevant details. Ensure compliance with relevant privacy laws, regulations and standards.\n",
    "\n",
    "10. **Additional Security Measures**: Implement all necessary security measures to protect against potential attacks or vulnerabilities. This may include encryption of data in transit and at rest, access controls, and auditing mechanisms. This may also include safeguards against side channel attacks such as isolated compute environments.\n",
    "\n",
    "11. **User Education and Training**: Educate users and stakeholders about the principles of differential privacy, its implications, and the importance of preserving privacy while conducting data analysis.\n",
    "\n",
    "12. **Continuous Monitoring and Maintenance**: Establish a process for ongoing monitoring and maintenance of the deployed system. Regularly review privacy parameters and update mechanisms as needed to adapt to evolving privacy requirements and threats.\n",
    "\n",
    "By following this checklist, you can effectively deploy a system with differential privacy to protect sensitive data while enabling valuable analysis and insights!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
