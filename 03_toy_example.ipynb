{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Walkthrough: generate(toy_df, epsilon=...)\n",
    "\n",
    "This notebook explains the generate function step by step using a small toy dataset.\n",
    "The goal is that after reading the Markdown cells, it is clear what each part of the code does, and what intermediate objects look like (histograms, classTotals, p, classTuples, cond_attr, attr_vectors, ...)."
   ],
   "id": "d0225cbfe3fdbf07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:20:04.693229Z",
     "start_time": "2025-12-15T10:20:03.266935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Hilfsfunktionen ----------\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity/epsilon)\n",
    "\n",
    "def _build_noisy_histograms(df, className, attributesName, epsilon):\n",
    "    \"\"\"\n",
    "    Phase 1: Baue f√ºr jedes Attribut ein DP-noisy Histogramm P(attr, class).\n",
    "    \"\"\"\n",
    "    numHistograms = len(attributesName)\n",
    "    epsilon_per_hist = epsilon / numHistograms\n",
    "    histograms = {}\n",
    "\n",
    "    for attributeName in attributesName:\n",
    "        counts = df[[attributeName, className]].value_counts()\n",
    "        noisy_counts = laplace_mech(counts, sensitivity=1, epsilon=epsilon_per_hist)\n",
    "        noisy_counts.clip(lower=0.0, inplace=True)\n",
    "        histograms[attributeName] = noisy_counts\n",
    "\n",
    "    return histograms\n",
    "\n",
    "\n",
    "def _compute_class_distribution(histograms, numTuples):\n",
    "    \"\"\"\n",
    "    Aus allen Histogrammen: classTotals, Klassenwahrscheinlichkeiten p(c),\n",
    "    Anzahl synthetischer Tupel pro Klasse und Klassenvektoren.\n",
    "    \"\"\"\n",
    "    classTotals = {}\n",
    "\n",
    "    # classTotals[c] = Summe √ºber alle Attribute und Attribute-Werte der Counts\n",
    "    for attr_name, hist in histograms.items():\n",
    "        for (attr_val, class_val), count in hist.items():\n",
    "            classTotals[class_val] = classTotals.get(class_val, 0.0) + count\n",
    "\n",
    "    total = sum(classTotals.values())\n",
    "    if total == 0:\n",
    "        # Fallback: gleichverteilt, falls durch Noise alles 0 wurde\n",
    "        gleich = 1.0 / len(classTotals)\n",
    "        p = {c: gleich for c in classTotals}\n",
    "    else:\n",
    "        p = {c: classTotals[c] / total for c in classTotals}\n",
    "\n",
    "    # Anzahl Tupel pro Klasse\n",
    "    classTuples = {c: round(numTuples * p[c]) for c in classTotals}\n",
    "\n",
    "    # Klassenvektor f√ºr jede Klasse\n",
    "    class_vector = {c: [c] * classTuples[c] for c in classTuples}\n",
    "\n",
    "    return classTotals, p, classTuples, class_vector\n",
    "\n",
    "\n",
    "def _compute_conditional_attributes(histograms, classTotals):\n",
    "    \"\"\"\n",
    "    Berechne P(attr = a | class = c) f√ºr jedes Attribut und jede Klasse.\n",
    "    \"\"\"\n",
    "    cond_attr = {}\n",
    "\n",
    "    for attr_name, hist in histograms.items():\n",
    "        cond_attr[attr_name] = {}\n",
    "        for c in classTotals:\n",
    "            attr_counts = {}\n",
    "\n",
    "            # Z√§hle f√ºr fixe Klasse c die H√§ufigkeiten der Attribut-Werte\n",
    "            for (attr_val, class_val), count in hist.items():\n",
    "                if class_val == c:\n",
    "                    attr_counts[attr_val] = attr_counts.get(attr_val, 0.0) + count\n",
    "\n",
    "            sum_c = sum(attr_counts.values())\n",
    "\n",
    "            if sum_c == 0:\n",
    "                # gleichverteilte Notl√∂sung\n",
    "                if attr_counts:  # Klasseninfo vorhanden, aber alles 0\n",
    "                    gleich = 1.0 / len(attr_counts)\n",
    "                    probs = {a: gleich for a in attr_counts}\n",
    "                else:\n",
    "                    # gar keine Werte ‚Äì leeres Dict, kann sp√§ter geskippt werden\n",
    "                    probs = {}\n",
    "            else:\n",
    "                probs = {a: attr_counts[a] / sum_c for a in attr_counts}\n",
    "\n",
    "            cond_attr[attr_name][c] = probs\n",
    "\n",
    "    return cond_attr\n",
    "\n",
    "\n",
    "def _sample_attribute_vectors(cond_attr, classTuples, random_state=None):\n",
    "    \"\"\"\n",
    "    Ziehe f√ºr jedes Attribut und jede Klasse einen Vektor von Attributwerten\n",
    "    mit L√§nge n_c = classTuples[c] entsprechend P(attr | class).\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    attr_vectors = {}\n",
    "\n",
    "    for attr_name, class_dict in cond_attr.items():\n",
    "        attr_vectors[attr_name] = {}\n",
    "        for c, probs in class_dict.items():\n",
    "            n_c = classTuples[c]\n",
    "\n",
    "            # Falls keine Wahrscheinlichkeiten existieren (leeres Dict)\n",
    "            if not probs:\n",
    "                # Notl√∂sung: Vektor komplett leer, wird sp√§ter ggf. ersetzt/geskippt\n",
    "                attr_vectors[attr_name][c] = [None] * n_c\n",
    "                continue\n",
    "\n",
    "            # Zielanzahl pro Attributwert\n",
    "            target_count = {attr_val: round(p_val * n_c)\n",
    "                            for attr_val, p_val in probs.items()}\n",
    "\n",
    "            # Baue den Vektor mit der Zielanzahl pro Wert\n",
    "            attr_vec_c = []\n",
    "            for attr_val, cnt in target_count.items():\n",
    "                attr_vec_c.extend([attr_val] * cnt)\n",
    "\n",
    "            # L√§nge anpassen (auf n_c)\n",
    "            current_len = len(attr_vec_c)\n",
    "            diff = n_c - current_len\n",
    "\n",
    "            if diff > 0:\n",
    "                vals = list(probs.keys())\n",
    "                extra = np.random.choice(vals, size=diff, p=list(probs.values()))\n",
    "                attr_vec_c.extend(extra)\n",
    "            elif diff < 0:\n",
    "                remove_indices = np.random.choice(len(attr_vec_c), size=-diff, replace=False)\n",
    "                for idx in sorted(remove_indices, reverse=True):\n",
    "                    attr_vec_c.pop(idx)\n",
    "\n",
    "            # Shuffle, um keine Struktur / Reihenfolge zu verraten\n",
    "            np.random.shuffle(attr_vec_c)\n",
    "\n",
    "            attr_vectors[attr_name][c] = attr_vec_c\n",
    "\n",
    "    return attr_vectors\n",
    "\n",
    "\n",
    "def _assemble_synthetic_dataframe(attr_vectors, class_vector, attributesName, className):\n",
    "    \"\"\"\n",
    "    Setze aus Attributvektoren und Klassenvektoren die DataFrames pro Klasse\n",
    "    und f√ºhre sie zu einem Gesamt-DataFrame zusammen.\n",
    "    \"\"\"\n",
    "    blocks = {}\n",
    "    for c, class_vec in class_vector.items():\n",
    "        n_c = len(class_vec)\n",
    "        block = {}\n",
    "\n",
    "        for attributeName in attributesName:\n",
    "            values = attr_vectors[attributeName][c]\n",
    "            # Sanity-Check: auf L√§nge n_c trimmen/auff√ºllen falls n√∂tig\n",
    "            if len(values) < n_c:\n",
    "                values = values + [values[-1]] * (n_c - len(values))\n",
    "            elif len(values) > n_c:\n",
    "                values = values[:n_c]\n",
    "            block[attributeName] = values\n",
    "\n",
    "        block[className] = class_vec\n",
    "        blocks[c] = block\n",
    "\n",
    "    df_blocks = {c: pd.DataFrame(block) for c, block in blocks.items()}\n",
    "    synthetic_df = pd.concat(df_blocks.values(), ignore_index=True)\n",
    "\n",
    "    return synthetic_df\n",
    "\n",
    "\n",
    "# ---------- Hauptfunktion ----------\n",
    "\n",
    "def generate(df, numTuples=None, numClass=None, epsilon=1.0, random_state=42):\n",
    "    \"\"\"\n",
    "    Erzeuge differentially private synthetische Daten auf Basis von df.\n",
    "    Phase 1: DP-Histogramme.\n",
    "    Phase 2: Ziehen von Klassen- und Attributwerten und Zusammenbau des synthetischen Datensatzes.\n",
    "    \"\"\"\n",
    "    print(df.shape)\n",
    "\n",
    "    # optionale Parameter abkl√§ren\n",
    "    if numTuples is None:\n",
    "        numTuples = df.shape[0]\n",
    "    if numClass is None:\n",
    "        numClass = df.shape[1] - 1\n",
    "\n",
    "    className = df.columns[numClass]\n",
    "    attributesName = [col for col in df.columns if col != className]\n",
    "\n",
    "    # ---- Phase 1: DP-Histogramme ----\n",
    "    histograms = _build_noisy_histograms(df, className, attributesName, epsilon)\n",
    "\n",
    "    # ---- Phase 2: Klassenverteilung, P(attr|class), Sampling, Zusammenbau ----\n",
    "    classTotals, p, classTuples, class_vector = _compute_class_distribution(\n",
    "        histograms, numTuples\n",
    "    )\n",
    "\n",
    "    cond_attr = _compute_conditional_attributes(histograms, classTotals)\n",
    "\n",
    "    attr_vectors = _sample_attribute_vectors(\n",
    "        cond_attr, classTuples, random_state=random_state\n",
    "    )\n",
    "\n",
    "    synthetic_df = _assemble_synthetic_dataframe(\n",
    "        attr_vectors, class_vector, attributesName, className\n",
    "    )\n",
    "\n",
    "    # final shuffle des gesamten DataFrames (Zeilen)\n",
    "    synthetic_df = synthetic_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return synthetic_df\n",
    "\n",
    "\n"
   ],
   "id": "6a68e040dabcd31e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1) Toy dataset and function call\n",
    "\n",
    "We use a small dataset with:\n",
    "\n",
    "two attributes: job, education\n",
    "\n",
    "one class variable: income\n",
    "\n",
    "In the next code cell, we construct toy_df and then call:\n",
    "\n",
    "`synthetic_toy = generate(toy_df, epsilon=1.0)`\n",
    "\n",
    "Epsilon is the privacy parameter, with 1.0 being the default value."
   ],
   "id": "851b9fd7d939998b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:23:59.902359Z",
     "start_time": "2025-12-15T10:23:59.895039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "toy_df = pd.DataFrame({\n",
    "    \"job\": [\n",
    "        \"blue\", \"blue\", \"white\",\n",
    "        \"white\", \"blue\", \"white\"\n",
    "    ],\n",
    "    \"education\": [\n",
    "        \"low\", \"low\", \"high\",\n",
    "        \"high\", \"high\", \"low\"\n",
    "    ],\n",
    "    \"income\": [\n",
    "        \"<=50K\", \"<=50K\", \">50K\",\n",
    "        \">50K\", \"<=50K\", \">50K\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"toy: \\n\",toy_df)\n",
    "\n",
    "synthetic_toy = generate(toy_df, epsilon=1.0)\n",
    "\n",
    "print(\"synthetic_toy: \\n\",synthetic_toy)"
   ],
   "id": "7ecc6582fb752da6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toy: \n",
      "      job education income\n",
      "0   blue       low  <=50K\n",
      "1   blue       low  <=50K\n",
      "2  white      high   >50K\n",
      "3  white      high   >50K\n",
      "4   blue      high  <=50K\n",
      "5  white       low   >50K\n",
      "(6, 3)\n",
      "synthetic_toy: \n",
      "      job education income\n",
      "0   blue       low  <=50K\n",
      "1   blue      high  <=50K\n",
      "2  white       low   >50K\n",
      "3   blue      high  <=50K\n",
      "4  white       low   >50K\n",
      "5  white      high   >50K\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2) generate(...): parameters and defaults\n",
    "\n",
    "The function signature is:\n",
    "\n",
    "`generate(df, numTuples=None, numClass=None, epsilon=1.0, ...)`\n",
    "\n",
    "Meaning:\n",
    "\n",
    "- **df**: the input DataFrame (here: toy_df)\n",
    "\n",
    "- **numTuples**: number of rows to generate\n",
    "\n",
    "- **numClass**: index of the class column\n",
    "\n",
    "numTuples and numClass can be set to not use certain parts of the DataFrame, the default is set to the total number of rows and the last column respectively.\n",
    "\n",
    "- **epsilon**: overall privacy budget\n",
    "\n",
    "it is split across attributes: $\\varepsilon_{\\text{per hist}} = \\frac{\\varepsilon}{\\text{no. of attributes}}$\n",
    "\n",
    "\n",
    "\n",
    "In our toy dataset:\n",
    "\n",
    "- number of rows: N=6\n",
    "\n",
    "- class column is the last column (income)\n",
    "\n",
    "- attributes are the other columns (job, education)\n",
    "\n",
    "- number of histograms: 2 ‚Üí ùúÄ per hist=ùúÄ/2"
   ],
   "id": "e6feb417648f4a5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:29:28.152859Z",
     "start_time": "2025-12-15T10:29:28.149322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = toy_df\n",
    "\n",
    "# same default logic as in generate\n",
    "numTuples = None\n",
    "numClass = None\n",
    "epsilon = 1.0\n",
    "\n",
    "if numTuples is None:\n",
    "    numTuples = df.shape[0]\n",
    "\n",
    "if numClass is None:\n",
    "    numClass = df.shape[1] - 1\n",
    "\n",
    "className = df.columns[numClass]\n",
    "attributesName = [col for col in df.columns if col != className]\n",
    "\n",
    "numHistograms = len(attributesName)\n",
    "epsilon_per_hist = epsilon / numHistograms\n",
    "\n",
    "print(\"df.shape:\", df.shape)\n",
    "print(\"numTuples:\", numTuples)\n",
    "print(\"className:\", className)\n",
    "print(\"attributesName:\", attributesName)\n",
    "print(\"epsilon_per_hist:\", epsilon_per_hist)"
   ],
   "id": "17c00fff83df247a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape: (6, 3)\n",
      "numTuples: 6\n",
      "className: income\n",
      "attributesName: ['job', 'education']\n",
      "epsilon_per_hist: 0.5\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3) Phase 1: building joint histograms (attribute, class)\n",
    "\n",
    "The first main block of code constructs a dictionary of histograms.\n",
    "\n",
    "### 3.1 What does one histogram represent?\n",
    "\n",
    "For each attribute (e.g. job), the code computes:\n",
    "\n",
    "`counts = df[[attributeName, className]].value_counts()`\n",
    "\n",
    "This returns a pandas Series with:\n",
    "\n",
    "- a MultiIndex: `(attr_val, class_val)`\n",
    "\n",
    "- values: `count(attr_val, class_val)`\n",
    "\n",
    "So for job, keys look like:\n",
    "\n",
    "(\"blue\", \"‚â§50K\")\n",
    "\n",
    "(\"white\", \">50K\")\n",
    "\n",
    "and values are the corresponding frequencies in the toy dataset.\n",
    "\n",
    "### 3.2 What is stored in histograms?\n",
    "\n",
    "histograms is a dict:\n",
    "\n",
    "**key**: attribute name (e.g. \"job\")\n",
    "\n",
    "**value**: noisy Series of counts with MultiIndex"
   ],
   "id": "60bc7e6ee45bc20e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:30:30.217131Z",
     "start_time": "2025-12-15T10:30:30.211663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Raw (non-noisy) joint histograms from the toy dataset\n",
    "\n",
    "job_income_hist = toy_df[[\"job\", \"income\"]].value_counts()\n",
    "edu_income_hist = toy_df[[\"education\", \"income\"]].value_counts()\n",
    "\n",
    "job_income_hist, edu_income_hist\n"
   ],
   "id": "665a47953a8bfc63",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(job    income\n",
       " blue   <=50K     3\n",
       " white  >50K      3\n",
       " Name: count, dtype: int64,\n",
       " education  income\n",
       " high       >50K      2\n",
       " low        <=50K     2\n",
       " high       <=50K     1\n",
       " low        >50K      1\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The outputs above are pandas Series with a MultiIndex.\n",
    "\n",
    "Each entry represents a joint count of the form:\n",
    "\n",
    "$(attribute value , class value ) ‚ü∂ count$\n",
    "\n",
    "For example, in the histogram for job:\n",
    "\n",
    "the index (\"blue\", \"<=50K\") with value 3 means\n",
    "that there are three records in the toy dataset with\n",
    "job = \"blue\" and income = \"<=50K\".\n",
    "\n",
    "All combinations that do not occur in the dataset are absent from the index and implicitly have count zero.\n",
    "\n",
    "These joint histograms contain all information used by the algorithm in Phase 1, before differential privacy is applied."
   ],
   "id": "6b97c7d89b7be45a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Phase 1 continued: Laplace noise and clipping\n",
    "\n",
    "After computing the true counts, the code applies the Laplace mechanism:\n",
    "\n",
    "`noisy_counts = laplace_mech(counts, 1, epsilon_per_hist)`\n",
    "\n",
    "Then:\n",
    "\n",
    "`noisy_counts.clip(lower=0.0, inplace=True)`\n",
    "\n",
    "Meaning:\n",
    "\n",
    "- each count is perturbed by Laplace noise\n",
    "\n",
    "- negative noisy counts are clipped to zero\n",
    "\n",
    "This is the only differentially private step in the pipeline.\n",
    "Everything after that is post-processing (no privacy cost)."
   ],
   "id": "6d229e85136409be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:37:39.071969Z",
     "start_time": "2025-12-15T10:37:39.065130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epsilon = 1.0\n",
    "attributesName = [\"job\", \"education\"]\n",
    "className = \"income\"\n",
    "\n",
    "numHistograms = len(attributesName)\n",
    "epsilon_per_hist = epsilon / numHistograms\n",
    "\n",
    "histograms = {}\n",
    "\n",
    "for attributeName in attributesName:\n",
    "    counts = toy_df[[attributeName, className]].value_counts()\n",
    "    noisy_counts = laplace_mech(counts, sensitivity=1, epsilon=epsilon_per_hist)\n",
    "    noisy_counts.clip(lower=0.0, inplace=True)\n",
    "    histograms[attributeName] = noisy_counts\n",
    "\n",
    "# Display noisy histograms\n",
    "histograms[\"job\"], histograms[\"education\"]\n"
   ],
   "id": "403467d349230e6f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(job    income\n",
       " blue   <=50K     7.315299\n",
       " white  >50K      7.315299\n",
       " Name: count, dtype: float64,\n",
       " education  income\n",
       " high       >50K      2.270669\n",
       " low        <=50K     2.270669\n",
       " high       <=50K     1.270669\n",
       " low        >50K      1.270669\n",
       " Name: count, dtype: float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Note**\n",
    "\n",
    "Since the toy dataset contains only six records, the variance introduced by the Laplace mechanism is large relative to the true counts.\n",
    "As the Laplace noise scale is independent of the dataset size, this effect diminishes for larger datasets."
   ],
   "id": "772d827ab88e9eca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5) Phase 2 starts: computing classTotals\n",
    "\n",
    "The next block computes classTotals, a dictionary mapping:\n",
    "\n",
    "**key**: class label c\n",
    "\n",
    "**value**: total noisy mass associated with class c\n",
    "\n",
    "This is done by iterating through all noisy histograms:\n",
    "\n",
    "- outer loop: over attributes (`for attr_name, hist in histograms.items()`)\n",
    "\n",
    "- inner loop: over MultiIndex entries (`for (attr_val, class_val), count in hist.items()`)\n",
    "\n",
    "For each entry, the noisy count is added to:\n",
    "\n",
    "`classTotals[class_val] += count`\n",
    "\n",
    "So conceptually:\n",
    "\n",
    "$$\n",
    "\\text{classTotals}(c)\n",
    "=\n",
    "\\sum_{A \\in \\mathcal{A}}\n",
    "\\sum_{a}\n",
    "\\tilde{N}(A = a, C = c)\n",
    "$$"
   ],
   "id": "c51cd1e810c90445"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:44:04.372111Z",
     "start_time": "2025-12-15T10:44:04.368666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classTotals = {}\n",
    "\n",
    "for attr_name, hist in histograms.items():\n",
    "    for (attr_val, class_val), count in hist.items():\n",
    "        if class_val not in classTotals:\n",
    "            classTotals[class_val] = 0.0\n",
    "        classTotals[class_val] += count\n",
    "\n",
    "total = sum(classTotals.values())\n",
    "\n",
    "classTotals, total\n"
   ],
   "id": "3e89b3be09beaf95",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<=50K': 10.856638030035528, '>50K': 10.856638030035528}, 21.713276060071056)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6) From classTotals to p and classTuples\n",
    "\n",
    "Next, the code computes a probability distribution over classes:\n",
    "\n",
    "`p = {c: classTotals[c] / total for c in classTotals}`\n",
    "\n",
    "$P(C=c)=\\frac{\\text{classTotals}(c)}{\\sum_{c'} \\text{classTotals}(c')}$\n",
    "\n",
    "\n",
    "Then, the number of synthetic tuples per class is:\n",
    "\n",
    "`classTuples = {c: round(numTuples * p[c]) for c in classTotals}`\n",
    "\n",
    "So: $n_c = \\text{round}\\bigl(N \\cdot P(C=c)\\bigr)$\n",
    "\n",
    "Finally, a class vector is built:\n",
    "\n",
    "`class_vector[c] = [c] * classTuples[c]`\n",
    "\n",
    "This fixes the class labels of the synthetic dataset before sampling the attributes."
   ],
   "id": "26ebedb1b25e9b65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:48:55.341213Z",
     "start_time": "2025-12-15T10:48:55.337137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute class probabilities p(C=c)\n",
    "p = {c: classTotals[c] / total for c in classTotals}\n",
    "\n",
    "# Number of synthetic tuples per class\n",
    "numTuples = toy_df.shape[0]\n",
    "classTuples = {c: round(numTuples * p[c]) for c in classTotals}\n",
    "\n",
    "# Class vectors (fixed labels for synthetic data)\n",
    "class_vector = {c: [c] * classTuples[c] for c in classTuples}\n",
    "\n",
    "p, classTuples, class_vector"
   ],
   "id": "7383a3c7c9875965",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<=50K': 0.5, '>50K': 0.5},\n",
       " {'<=50K': 3, '>50K': 3},\n",
       " {'<=50K': ['<=50K', '<=50K', '<=50K'], '>50K': ['>50K', '>50K', '>50K']})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7) Conditional distributions `cond_attr[attr][class]`\n",
    "\n",
    "This step computes the conditional distributions of attribute values given the class.\n",
    "\n",
    "For each attribute \\( A \\) and each class \\( c \\), the algorithm estimates:\n",
    "\n",
    "$P(A = a \\mid C = c)=\\frac{\\tilde{N}(A = a, C = c)}{\\sum_{a'} \\tilde{N}(A = a', C = c)}$\n",
    "\n",
    "where $\\tilde{N}(A = a, C = c)$ denotes the noisy histogram counts obtained in Phase 1.\n",
    "\n",
    "### How `attr_counts` is computed\n",
    "\n",
    "For a fixed attribute `attr_name` and class \\( c \\), the code iterates over all\n",
    "entries of the corresponding noisy histogram:\n",
    "\n",
    "- only entries with `class_val == c` are considered,\n",
    "- the noisy counts are accumulated per attribute value:\n",
    "  `attr_counts[attr_val] += count`.\n",
    "\n",
    "This yields a dictionary mapping attribute values \\( a \\) to their total noisy\n",
    "mass within class \\( c \\).\n",
    "\n",
    "### Normalization and edge cases\n",
    "\n",
    "Let:\n",
    "\n",
    "$\\text{sum}_c = \\sum_a \\tilde{N}(A = a, C = c)$\n",
    "\n",
    "- If $\\text{sum}_c > 0$, the conditional probabilities are obtained by normalization.\n",
    "- If $\\text{sum}_c = 0$ but attribute values are present, a uniform distribution\n",
    "  over the observed attribute values is used.\n",
    "- If no information is available at all, an empty distribution is stored.\n",
    "\n",
    "The result is stored in:\n",
    "\n",
    "`cond_attr[attr_name][c]`\n",
    "\n",
    "which is later used for sampling synthetic attribute values.\n"
   ],
   "id": "9d0f59ab1acf150a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:50:23.047703Z",
     "start_time": "2025-12-15T10:50:23.043491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute conditional attribute distributions cond_attr\n",
    "cond_attr = {}\n",
    "\n",
    "for attr_name, hist in histograms.items():\n",
    "    cond_attr[attr_name] = {}\n",
    "    for c in classTotals:\n",
    "        attr_counts = {}\n",
    "\n",
    "        for (attr_val, class_val), count in hist.items():\n",
    "            if class_val == c:\n",
    "                attr_counts[attr_val] = attr_counts.get(attr_val, 0.0) + count\n",
    "\n",
    "        sum_c = sum(attr_counts.values())\n",
    "\n",
    "        if sum_c == 0:\n",
    "            if attr_counts:\n",
    "                gleich = 1.0 / len(attr_counts)\n",
    "                probs = {a: gleich for a in attr_counts}\n",
    "            else:\n",
    "                probs = {}\n",
    "        else:\n",
    "            probs = {a: attr_counts[a] / sum_c for a in attr_counts}\n",
    "\n",
    "        cond_attr[attr_name][c] = probs\n",
    "\n",
    "cond_attr"
   ],
   "id": "9b7dc6d85c19219a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job': {'<=50K': {'blue': 1.0}, '>50K': {'white': 1.0}},\n",
       " 'education': {'<=50K': {'low': 0.6411895477322098,\n",
       "   'high': 0.3588104522677903},\n",
       "  '>50K': {'high': 0.6411895477322098, 'low': 0.3588104522677903}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8) Sampling: attr_vectors[attr][class]\n",
    "\n",
    "Now we generate concrete vectors of attribute values.\n",
    "\n",
    "For each attribute and each class:\n",
    "\n",
    "- set `sum_c = classTuples[c]`\n",
    "\n",
    "- compute target counts:\n",
    "`target_count[attr_val] = round(p_val * n_c)`\n",
    "\n",
    "- construct a vector by repeating each attr_val target_count[attr_val] times\n",
    "\n",
    "Then the code enforces that the vector length is exactly nc\n",
    "\n",
    "if too short ‚Üí sample extra values using `np.random.choice(..., p=...)`\n",
    "\n",
    "if too long ‚Üí remove random entries\n",
    "\n",
    "Finally:\n",
    "`np.random.shuffle(attr_vec_c)`\n",
    "\n",
    "This prevents ordering artifacts."
   ],
   "id": "cd5f3d7290c2c730"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:01:49.780778Z",
     "start_time": "2025-12-15T11:01:49.777568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build attr_vectors (sampling step)\n",
    "attr_vectors = {}\n",
    "\n",
    "for attr_name, class_dict in cond_attr.items():\n",
    "    attr_vectors[attr_name] = {}\n",
    "    for c, probs in class_dict.items():\n",
    "        n_c = classTuples[c]\n",
    "        target_count = {}\n",
    "\n",
    "        for attr_val, p_val in probs.items():\n",
    "            target_count[attr_val] = round(p_val * n_c)\n",
    "\n",
    "        attr_vec_c = []\n",
    "        for attr_val, cnt in target_count.items():\n",
    "            attr_vec_c.extend([attr_val] * cnt)\n",
    "\n",
    "        current_len = len(attr_vec_c)\n",
    "        diff = n_c - current_len\n",
    "\n",
    "        if diff > 0:\n",
    "            vals = list(probs.keys())\n",
    "            extra = np.random.choice(vals, size=diff, p=list(probs.values()))\n",
    "            attr_vec_c.extend(extra)\n",
    "        elif diff < 0:\n",
    "            remove_indices = np.random.choice(len(attr_vec_c), size=-diff, replace=False)\n",
    "            for idx in sorted(remove_indices, reverse=True):\n",
    "                attr_vec_c.pop(idx)\n",
    "\n",
    "        np.random.shuffle(attr_vec_c)\n",
    "        attr_vectors[attr_name][c] = attr_vec_c"
   ],
   "id": "ae0e08ad089680fe",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:01:52.175246Z",
     "start_time": "2025-12-15T11:01:52.172312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspect sampled attribute vectors for the toy dataset\n",
    "\n",
    "for c in classTuples:\n",
    "    print(f\"Class: {c}\")\n",
    "\n",
    "    print(\" job:\")\n",
    "    print(\"  length:\", len(attr_vectors[\"job\"][c]))\n",
    "    print(\"  example:\", attr_vectors[\"job\"][c][:10])\n",
    "\n",
    "    print(\" education:\")\n",
    "    print(\"  length:\", len(attr_vectors[\"education\"][c]))\n",
    "    print(\"  example:\", attr_vectors[\"education\"][c][:10])\n",
    "    print()\n"
   ],
   "id": "30bbb2f014d70f20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: <=50K\n",
      " job:\n",
      "  length: 3\n",
      "  example: ['blue', 'blue', 'blue']\n",
      " education:\n",
      "  length: 3\n",
      "  example: ['low', 'high', 'low']\n",
      "\n",
      "Class: >50K\n",
      " job:\n",
      "  length: 3\n",
      "  example: ['white', 'white', 'white']\n",
      " education:\n",
      "  length: 3\n",
      "  example: ['high', 'high', 'low']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9) Building blocks, concatenating, final shuffle\n",
    "\n",
    "At this point we have:\n",
    "\n",
    "- `class_vector[c]` containing class labels (length $\\text{n}_c$)\n",
    "\n",
    "- `attr_vectors[attr][c]` for each attribute (also length $\\text{n}_c$)\n",
    "\n",
    "The code builds one ‚Äúblock‚Äù per class:\n",
    "\n",
    "each block is a dict:\n",
    "\n",
    "- `block[attributeName] = attr_vectors[attributeName][c]`\n",
    "\n",
    "- `block[className] = class_vector[c]`\n",
    "\n",
    "Then:\n",
    "\n",
    "`df_blocks[c] = pd.DataFrame(block)` creates one DataFrame per class\n",
    "\n",
    "`synthetic_df = pd.concat(df_blocks.values(), ignore_index=True)` merges them\n",
    "\n",
    "`synthetic_df = synthetic_df.sample(frac=1, random_state=42)` shuffles rows"
   ],
   "id": "9294e6d01a26fe7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T11:04:45.017356Z",
     "start_time": "2025-12-15T11:04:45.010829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build class-wise blocks and assemble the final synthetic DataFrame\n",
    "blocks = {}\n",
    "\n",
    "for c in classTuples:\n",
    "    block = {}\n",
    "    for attributeName in attributesName:\n",
    "        block[attributeName] = attr_vectors[attributeName][c]\n",
    "    block[className] = class_vector[c]\n",
    "    blocks[c] = block\n",
    "\n",
    "# Create one DataFrame per class\n",
    "df_blocks = {c: pd.DataFrame(block) for c, block in blocks.items()}\n",
    "\n",
    "# Concatenate all class DataFrames\n",
    "synthetic_df = pd.concat(df_blocks.values(), ignore_index=True)\n",
    "\n",
    "# Final shuffle of rows\n",
    "synthetic_df = synthetic_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "synthetic_df.head(), synthetic_df[\"income\"].value_counts(), toy_df[\"income\"].value_counts()"
   ],
   "id": "2db7aad67bb392e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     job education income\n",
       " 0   blue       low  <=50K\n",
       " 1   blue      high  <=50K\n",
       " 2  white       low   >50K\n",
       " 3   blue       low  <=50K\n",
       " 4  white      high   >50K,\n",
       " income\n",
       " <=50K    3\n",
       " >50K     3\n",
       " Name: count, dtype: int64,\n",
       " income\n",
       " <=50K    3\n",
       " >50K     3\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10) Interpretation: what is preserved and what can change?\n",
    "\n",
    "Even on this toy dataset, we should expect:\n",
    "\n",
    "Typically preserved (in expectation):\n",
    "\n",
    "- the overall class balance $\\text{P(C)}$\n",
    "\n",
    "- conditional structure $\\text{P(A|C)}$\n",
    "\n",
    "Allowed to change due to DP noise:\n",
    "\n",
    "- exact histogram counts\n",
    "\n",
    "- exact correlations / small-sample artifacts\n",
    "\n",
    "- some rare combinations may disappear or appear\n",
    "\n",
    "This is the intended privacy‚Äìutility tradeoff controlled by Œµ."
   ],
   "id": "da6db4b54f3ceddf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11) Connection to the full experimental study\n",
    "\n",
    "This notebook demonstrates the mechanics of the pipeline with a tiny dataset.\n",
    "In the full study, the exact same function is applied to real datasets, and we evaluate utility via classification performance for multiple\n",
    "Œµ values with repeated runs.\n",
    "\n",
    "The toy walkthrough is meant to make the experimental pipeline fully understandable."
   ],
   "id": "2c4ca1496119a819"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
