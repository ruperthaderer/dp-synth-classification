{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-20T22:34:46.637650Z",
     "start_time": "2025-12-20T22:34:45.624521Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "data = pd.read_csv('data\\\\adult_mrf.csv')\n",
    "\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity/epsilon)\n",
    "\n",
    "data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0                 1  2          3   4                   5  \\\n",
       "0  4         State-gov  0  Bachelors  13       Never-married   \n",
       "1  7  Self-emp-not-inc  0  Bachelors  13  Married-civ-spouse   \n",
       "2  4           Private  1    HS-grad   9            Divorced   \n",
       "3  7           Private  2       11th   7  Married-civ-spouse   \n",
       "4  2           Private  3  Bachelors  13  Married-civ-spouse   \n",
       "\n",
       "                   6              7      8       9  10  11  12             13  \\\n",
       "0       Adm-clerical  Not-in-family  White    Male   0   0   6  United-States   \n",
       "1    Exec-managerial        Husband  White    Male   0   0   1  United-States   \n",
       "2  Handlers-cleaners  Not-in-family  White    Male   0   0   6  United-States   \n",
       "3  Handlers-cleaners        Husband  Black    Male   0   0   6  United-States   \n",
       "4     Prof-specialty           Wife  Black  Female   0   0   6           Cuba   \n",
       "\n",
       "      14  \n",
       "0  <=50K  \n",
       "1  <=50K  \n",
       "2  <=50K  \n",
       "3  <=50K  \n",
       "4  <=50K  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Private</td>\n",
       "      <td>1</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Private</td>\n",
       "      <td>2</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Private</td>\n",
       "      <td>3</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T22:34:55.777483Z",
     "start_time": "2025-12-20T22:34:55.714198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#generate Phase 1\n",
    "\n",
    "def generate(df, numTuples=None, numClass=None, epsilon=1.0):\n",
    "    print(df.shape)\n",
    "    #optionale parameter abklären\n",
    "    if numTuples is None:\n",
    "        numTuples = df.shape[0]\n",
    "    if numClass is None:\n",
    "        numClass = df.shape[1] - 1\n",
    "\n",
    "    className = df.columns[numClass]\n",
    "    attributesName = [col for col in df.columns if col != className]\n",
    "\n",
    "    numHistograms = df.shape[1] - 1\n",
    "    epsilon_per_hist = epsilon/numHistograms\n",
    "\n",
    "    histograms = {} # Dict für jedes Marginal nach Attribut sortiert\n",
    "    for attributeName in attributesName:\n",
    "        counts = df[[attributeName, className]].value_counts() # Series mit MultiIndex, Ebene 0 = Attribut, Ebene 1 = Klasse\n",
    "        noisy_counts = laplace_mech(counts, 1, epsilon_per_hist)\n",
    "        noisy_counts.clip(lower=0.0, inplace=True) # Entfernen von negativen Werten\n",
    "        #noisy_counts = noisy_counts.round()\n",
    "        histograms[attributeName] = noisy_counts\n",
    "\n",
    "    print(histograms[\"1\"])\n",
    "\n",
    "generate(data)\n"
   ],
   "id": "104d19097a9ff58d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 15)\n",
      "1                 14   \n",
      "Private           <=50K    26061.414814\n",
      "                  >50K      7256.414814\n",
      "Self-emp-not-inc  <=50K     2742.414814\n",
      "Local-gov         <=50K     2190.414814\n",
      "State-gov         <=50K     1431.414814\n",
      "Self-emp-not-inc  >50K      1064.414814\n",
      "Local-gov         >50K       920.414814\n",
      "Self-emp-inc      >50K       917.414814\n",
      "Federal-gov       <=50K      862.414814\n",
      "Self-emp-inc      <=50K      739.414814\n",
      "Federal-gov       >50K       554.414814\n",
      "State-gov         >50K       525.414814\n",
      "Without-pay       <=50K       24.414814\n",
      "                  >50K         7.414814\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T22:35:00.056910Z",
     "start_time": "2025-12-20T22:35:00.048510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Hilfsfunktionen ----------\n",
    "\n",
    "def _build_noisy_histograms(df, className, attributesName, epsilon):\n",
    "    \"\"\"\n",
    "    Phase 1: Baue für jedes Attribut ein DP-noisy Histogramm P(attr, class).\n",
    "    \"\"\"\n",
    "    numHistograms = len(attributesName)\n",
    "    epsilon_per_hist = epsilon / numHistograms\n",
    "    histograms = {}\n",
    "\n",
    "    for attributeName in attributesName:\n",
    "        counts = df[[attributeName, className]].value_counts()\n",
    "        noisy_counts = laplace_mech(counts, sensitivity=1, epsilon=epsilon_per_hist)\n",
    "        noisy_counts.clip(lower=0.0, inplace=True)\n",
    "        histograms[attributeName] = noisy_counts\n",
    "\n",
    "    return histograms\n",
    "\n",
    "\n",
    "def _compute_class_distribution(histograms, numTuples):\n",
    "    \"\"\"\n",
    "    Aus allen Histogrammen: classTotals, Klassenwahrscheinlichkeiten p(c),\n",
    "    Anzahl synthetischer Tupel pro Klasse und Klassenvektoren.\n",
    "    \"\"\"\n",
    "    classTotals = {}\n",
    "\n",
    "    # classTotals[c] = Summe über alle Attribute und Attribute-Werte der Counts\n",
    "    for attr_name, hist in histograms.items():\n",
    "        for (attr_val, class_val), count in hist.items():\n",
    "            classTotals[class_val] = classTotals.get(class_val, 0.0) + count\n",
    "\n",
    "    total = sum(classTotals.values())\n",
    "    if total == 0:\n",
    "        # Fallback: gleichverteilt, falls durch Noise alles 0 wurde\n",
    "        gleich = 1.0 / len(classTotals)\n",
    "        p = {c: gleich for c in classTotals}\n",
    "    else:\n",
    "        p = {c: classTotals[c] / total for c in classTotals}\n",
    "\n",
    "    # Anzahl Tupel pro Klasse\n",
    "    classTuples = {c: round(numTuples * p[c]) for c in classTotals}\n",
    "\n",
    "    # Klassenvektor für jede Klasse\n",
    "    class_vector = {c: [c] * classTuples[c] for c in classTuples}\n",
    "\n",
    "    return classTotals, p, classTuples, class_vector\n",
    "\n",
    "\n",
    "def _compute_conditional_attributes(histograms, classTotals):\n",
    "    \"\"\"\n",
    "    Berechne P(attr = a | class = c) für jedes Attribut und jede Klasse.\n",
    "    \"\"\"\n",
    "    cond_attr = {}\n",
    "\n",
    "    for attr_name, hist in histograms.items():\n",
    "        cond_attr[attr_name] = {}\n",
    "        for c in classTotals:\n",
    "            attr_counts = {}\n",
    "\n",
    "            # Zähle für fixe Klasse c die Häufigkeiten der Attribut-Werte\n",
    "            for (attr_val, class_val), count in hist.items():\n",
    "                if class_val == c:\n",
    "                    attr_counts[attr_val] = attr_counts.get(attr_val, 0.0) + count\n",
    "\n",
    "            sum_c = sum(attr_counts.values())\n",
    "\n",
    "            if sum_c == 0:\n",
    "                # gleichverteilte Notlösung\n",
    "                if attr_counts:  # Klasseninfo vorhanden, aber alles 0\n",
    "                    gleich = 1.0 / len(attr_counts)\n",
    "                    probs = {a: gleich for a in attr_counts}\n",
    "                else:\n",
    "                    # gar keine Werte – leeres Dict, kann später geskippt werden\n",
    "                    probs = {}\n",
    "            else:\n",
    "                probs = {a: attr_counts[a] / sum_c for a in attr_counts}\n",
    "\n",
    "            cond_attr[attr_name][c] = probs\n",
    "\n",
    "    return cond_attr\n",
    "\n",
    "\n",
    "def _sample_attribute_vectors(cond_attr, classTuples, random_state=None):\n",
    "    \"\"\"\n",
    "    Ziehe für jedes Attribut und jede Klasse einen Vektor von Attributwerten\n",
    "    mit Länge n_c = classTuples[c] entsprechend P(attr | class).\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    attr_vectors = {}\n",
    "\n",
    "    for attr_name, class_dict in cond_attr.items():\n",
    "        attr_vectors[attr_name] = {}\n",
    "        for c, probs in class_dict.items():\n",
    "            n_c = classTuples[c]\n",
    "\n",
    "            # Falls keine Wahrscheinlichkeiten existieren (leeres Dict)\n",
    "            if not probs:\n",
    "                # Notlösung: Vektor komplett leer, wird später ggf. ersetzt/geskippt\n",
    "                attr_vectors[attr_name][c] = [None] * n_c\n",
    "                continue\n",
    "\n",
    "            # Zielanzahl pro Attributwert\n",
    "            target_count = {attr_val: round(p_val * n_c)\n",
    "                            for attr_val, p_val in probs.items()}\n",
    "\n",
    "            # Baue den Vektor mit der Zielanzahl pro Wert\n",
    "            attr_vec_c = []\n",
    "            for attr_val, cnt in target_count.items():\n",
    "                attr_vec_c.extend([attr_val] * cnt)\n",
    "\n",
    "            # Länge anpassen (auf n_c)\n",
    "            current_len = len(attr_vec_c)\n",
    "            diff = n_c - current_len\n",
    "\n",
    "            if diff > 0:\n",
    "                vals = list(probs.keys())\n",
    "                extra = np.random.choice(vals, size=diff, p=list(probs.values()))\n",
    "                attr_vec_c.extend(extra)\n",
    "            elif diff < 0:\n",
    "                remove_indices = np.random.choice(len(attr_vec_c), size=-diff, replace=False)\n",
    "                for idx in sorted(remove_indices, reverse=True):\n",
    "                    attr_vec_c.pop(idx)\n",
    "\n",
    "            # Shuffle, um keine Struktur / Reihenfolge zu verraten\n",
    "            np.random.shuffle(attr_vec_c)\n",
    "\n",
    "            attr_vectors[attr_name][c] = attr_vec_c\n",
    "\n",
    "    return attr_vectors\n",
    "\n",
    "\n",
    "def _assemble_synthetic_dataframe(attr_vectors, class_vector, attributesName, className):\n",
    "    \"\"\"\n",
    "    Setze aus Attributvektoren und Klassenvektoren die DataFrames pro Klasse\n",
    "    und führe sie zu einem Gesamt-DataFrame zusammen.\n",
    "    \"\"\"\n",
    "    blocks = {}\n",
    "    for c, class_vec in class_vector.items():\n",
    "        n_c = len(class_vec)\n",
    "        block = {}\n",
    "\n",
    "        for attributeName in attributesName:\n",
    "            values = attr_vectors[attributeName][c]\n",
    "            # Sanity-Check: auf Länge n_c trimmen/auffüllen falls nötig\n",
    "            if len(values) < n_c:\n",
    "                values = values + [values[-1]] * (n_c - len(values))\n",
    "            elif len(values) > n_c:\n",
    "                values = values[:n_c]\n",
    "            block[attributeName] = values\n",
    "\n",
    "        block[className] = class_vec\n",
    "        blocks[c] = block\n",
    "\n",
    "    df_blocks = {c: pd.DataFrame(block) for c, block in blocks.items()}\n",
    "    synthetic_df = pd.concat(df_blocks.values(), ignore_index=True)\n",
    "\n",
    "    return synthetic_df\n",
    "\n",
    "\n",
    "# ---------- Hauptfunktion ----------\n",
    "\n",
    "def generate(df, numTuples=None, numClass=None, epsilon=1.0, random_state=42):\n",
    "    \"\"\"\n",
    "    Erzeuge differentially private synthetische Daten auf Basis von df.\n",
    "    Phase 1: DP-Histogramme.\n",
    "    Phase 2: Ziehen von Klassen- und Attributwerten und Zusammenbau des synthetischen Datensatzes.\n",
    "    \"\"\"\n",
    "    print(df.shape)\n",
    "\n",
    "    # optionale Parameter abklären\n",
    "    if numTuples is None:\n",
    "        numTuples = df.shape[0]\n",
    "    if numClass is None:\n",
    "        numClass = df.shape[1] - 1\n",
    "\n",
    "    className = df.columns[numClass]\n",
    "    attributesName = [col for col in df.columns if col != className]\n",
    "\n",
    "    # ---- Phase 1: DP-Histogramme ----\n",
    "    histograms = _build_noisy_histograms(df, className, attributesName, epsilon)\n",
    "\n",
    "    # ---- Phase 2: Klassenverteilung, P(attr|class), Sampling, Zusammenbau ----\n",
    "    classTotals, p, classTuples, class_vector = _compute_class_distribution(\n",
    "        histograms, numTuples\n",
    "    )\n",
    "\n",
    "    cond_attr = _compute_conditional_attributes(histograms, classTotals)\n",
    "\n",
    "    attr_vectors = _sample_attribute_vectors(\n",
    "        cond_attr, classTuples, random_state=random_state\n",
    "    )\n",
    "\n",
    "    synthetic_df = _assemble_synthetic_dataframe(\n",
    "        attr_vectors, class_vector, attributesName, className\n",
    "    )\n",
    "\n",
    "    # final shuffle des gesamten DataFrames (Zeilen)\n",
    "    synthetic_df = synthetic_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return synthetic_df\n",
    "\n",
    "# Beispiel-Aufruf:\n",
    "# synthetic_data = generate(data, epsilon=1.0)\n"
   ],
   "id": "982ce399b08d509a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T22:35:07.101339Z",
     "start_time": "2025-12-20T22:35:07.096217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#GPT Zelle\n",
    "def build_pipeline(clf=None):\n",
    "    \"\"\"\n",
    "    Baut eine sklearn-Pipeline:\n",
    "    - OneHotEncoding für alle Feature-Spalten (alle außer der letzten)\n",
    "    - Classifier (default: RandomForest, kann aber übergeben werden)\n",
    "    \"\"\"\n",
    "    if clf is None:\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # Alle Spalten außer der letzten sind Features\n",
    "    # (funktioniert auch mit '0', '1', ..., '13' als Spaltennamen)\n",
    "    def make_preprocessor(df):\n",
    "        feature_cols = df.columns[:-1]\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), feature_cols)\n",
    "            ]\n",
    "        )\n",
    "        return preprocessor\n",
    "\n",
    "    # kleine Wrapperfunktion, damit wir df nicht global brauchen\n",
    "    def make_model(df):\n",
    "        preprocessor = make_preprocessor(df)\n",
    "        model = Pipeline(steps=[\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", clf)\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    return make_model\n",
    "\n",
    "\n",
    "def evaluate_df(train_df, test_df=None, clf=None, n_splits=10, test_size=0.2, base_random_state=0):\n",
    "    \"\"\"\n",
    "    - train_df: DataFrame mit letzter Spalte = Klasse\n",
    "    - test_df: optionaler DataFrame, falls auf einem anderen DataFrame getestet wird\n",
    "    - clf: optionaler Classifier (sonst RandomForest)\n",
    "    - n_splits: wie viele verschiedene Train/Test-Splits (mit unterschiedlichen seeds)\n",
    "    - test_size: Anteil Testdaten\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = train_df.iloc[:, :-1]\n",
    "    y_train = train_df.iloc[:, -1]\n",
    "\n",
    "    if test_df is None:\n",
    "        X_test = train_df.iloc[:, :-1]\n",
    "        y_test = train_df.iloc[:, -1]\n",
    "    else:\n",
    "        X_test = test_df.iloc[:, :-1]\n",
    "        y_test = test_df.iloc[:, -1]\n",
    "\n",
    "    make_model = build_pipeline(clf)\n",
    "\n",
    "    accuracies = []\n",
    "    confusion_matrices = []\n",
    "\n",
    "    classes = np.unique(pd.concat([y_train, y_test], axis=0))\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        rs = base_random_state + i\n",
    "        if clf is None:\n",
    "            local_clf = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                random_state=42 + rs,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        else:\n",
    "            local_clf = clf\n",
    "\n",
    "        local_make_model = build_pipeline(local_clf)\n",
    "        model = local_make_model(train_df)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = (y_pred == y_test).mean()\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "        confusion_matrices.append(cm)\n",
    "\n",
    "    return {\n",
    "        \"classes\": classes,\n",
    "        \"accuracies\": np.array(accuracies),\n",
    "        \"confusion_matrices\": confusion_matrices,\n",
    "    }\n"
   ],
   "id": "a2b3a07b1420e0d1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Synthetische Daten\n",
    "synthetic_data = generate(data)\n",
    "synth_results = evaluate_df(synthetic_data, data, n_splits=10)\n",
    "\n",
    "# Originaldaten (data) aus CSV\n",
    "orig_results = evaluate_df(data, n_splits=10)\n",
    "\n",
    "print(\"Originaldaten:\")\n",
    "print(\"  Accuracy mean:\", orig_results[\"accuracies\"].mean())\n",
    "print(\"  Accuracy std :\", orig_results[\"accuracies\"].std())\n",
    "print(\"  Confusion Matrix (Split 0):\")\n",
    "print(orig_results[\"confusion_matrices\"][0])\n",
    "print(\"  Klassenreihenfolge:\", orig_results[\"classes\"])\n",
    "\n",
    "print(\"\\nSynthetische Daten:\")\n",
    "print(\"  Accuracy mean:\", synth_results[\"accuracies\"].mean())\n",
    "print(\"  Accuracy std :\", synth_results[\"accuracies\"].std())\n",
    "print(\"  Confusion Matrix (Split 0):\")\n",
    "print(synth_results[\"confusion_matrices\"][0])\n",
    "print(\"  Klassenreihenfolge:\", synth_results[\"classes\"])\n",
    "\n",
    "synthetic_data.head()"
   ],
   "id": "f436ed03d7c446f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "#version mit alternativem clf\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Synthetische Daten\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "synthetic_data = generate(data)   # deine Funktion\n",
    "synth_results = evaluate_df(synthetic_data, data, clf=clf, n_splits=10)\n",
    "\n",
    "# Originaldaten (data) aus CSV\n",
    "orig_results = evaluate_df(data, clf=clf, n_splits=10)\n",
    "\n",
    "print(\"Originaldaten:\")\n",
    "print(\"  Accuracy mean:\", orig_results[\"accuracies\"].mean())\n",
    "print(\"  Accuracy std :\", orig_results[\"accuracies\"].std())\n",
    "print(\"  Confusion Matrix (Split 0):\")\n",
    "print(orig_results[\"confusion_matrices\"][0])\n",
    "print(\"  Klassenreihenfolge:\", orig_results[\"classes\"])\n",
    "\n",
    "print(\"\\nSynthetische Daten:\")\n",
    "print(\"  Accuracy mean:\", synth_results[\"accuracies\"].mean())\n",
    "print(\"  Accuracy std :\", synth_results[\"accuracies\"].std())\n",
    "print(\"  Confusion Matrix (Split 0):\")\n",
    "print(synth_results[\"confusion_matrices\"][0])\n",
    "print(\"  Klassenreihenfolge:\", synth_results[\"classes\"])\n",
    "'''"
   ],
   "id": "7b03d8a215a8cd20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T03:52:18.815458Z",
     "start_time": "2025-12-20T22:35:26.053022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_epsilon_study(datasets,          # dict: {\"adult\": df_adult, \"bank\": df_bank, ...}\n",
    "                      epsilons,          # Liste der Epsilon-Werte\n",
    "                      n_reps=10,\n",
    "                      n_splits=10):\n",
    "    \"\"\"\n",
    "    Führt für alle Datensätze und alle Epsilon-Werte eine Experimental-Studie durch.\n",
    "    Gibt ein DataFrame mit Ergebnissen zurück.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for ds_name, df in datasets.items():\n",
    "        print(f\"Dataset: {ds_name}\")\n",
    "        for eps in epsilons:\n",
    "            for rep in range(n_reps):\n",
    "                # synthetische Daten generieren\n",
    "                synth = generate(df, epsilon=eps, random_state=42 + rep)\n",
    "\n",
    "                # Klassifikations-Performance auf synthetischen Daten messen\n",
    "                acc_synth = evaluate_df(synth, df, n_splits=n_splits)\n",
    "\n",
    "                # Optional: Performance auf Originaldaten (Baseline)\n",
    "                acc_orig = evaluate_df(df, n_splits=n_splits)\n",
    "\n",
    "                results.append({\n",
    "                    \"dataset\": ds_name,\n",
    "                    \"epsilon\": eps,\n",
    "                    \"rep\": rep,\n",
    "                    \"accuracy_synth\": acc_synth,\n",
    "                    \"accuracy_orig\": acc_orig\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "acs = pd.read_csv('data\\\\acs_mrf.csv')\n",
    "adult = pd.read_csv('data\\\\adult_mrf.csv')\n",
    "br2000 = pd.read_csv('data\\\\br2000_mrf.csv')\n",
    "# Beispiel-Setup:\n",
    "datasets = {\n",
    "    \"acs\": acs,\n",
    "    \"adult\": adult,\n",
    "    \"br2000\": br2000,\n",
    " }\n",
    "epsilons = [0.01, 0.1, 0.2, 0.4, 0.8, 1.0, 2.0, 4.0]\n",
    "study_results = run_epsilon_study(datasets, epsilons, n_reps=10, n_splits=10)\n"
   ],
   "id": "7e845ffcf19846b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: acs\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "Dataset: adult\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "Dataset: br2000\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T06:34:11.586326Z",
     "start_time": "2025-12-21T06:34:11.558015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "\n",
    "# Dict-Spalten -> mean-Accuracy-Spalten\n",
    "\n",
    "def extract_mean_accuracy(x):\n",
    "    # x kann schon ein dict oder ein String sein\n",
    "    if isinstance(x, dict):\n",
    "        d = x\n",
    "    else:\n",
    "        try:\n",
    "            d = ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    accs = d.get(\"accuracies\", None)\n",
    "    if accs is None or len(accs) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return float(np.mean(accs))\n",
    "\n",
    "study_results[\"acc_synth_mean\"] = study_results[\"accuracy_synth\"].apply(extract_mean_accuracy)\n",
    "study_results[\"acc_orig_mean\"]  = study_results[\"accuracy_orig\"].apply(extract_mean_accuracy)\n",
    "\n",
    "# Zusammenfassung pro Datensatz & Epsilon\n",
    "\n",
    "summary = (\n",
    "    study_results\n",
    "    .groupby([\"dataset\", \"epsilon\"])\n",
    "    .agg(\n",
    "        mean_acc_synth=(\"acc_synth_mean\", \"mean\"),\n",
    "        std_acc_synth=(\"acc_synth_mean\", \"std\"),\n",
    "        mean_acc_orig=(\"acc_orig_mean\", \"mean\"),\n",
    "        std_acc_orig=(\"acc_orig_mean\", \"std\"),\n",
    "        n_runs=(\"rep\", \"nunique\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary[\"diff_mean\"] = summary[\"mean_acc_synth\"] - summary[\"mean_acc_orig\"]\n",
    "summary = summary.sort_values([\"dataset\", \"epsilon\"])\n",
    "\n",
    "# Kurz ausgeben & (optional) Pivot-Tabellen für weitere Auswertung \n",
    "\n",
    "print(\"=== Zusammenfassung pro Dataset & Epsilon ===\")\n",
    "print(summary)\n",
    "\n",
    "pivot_synth = summary.pivot(index=\"epsilon\", columns=\"dataset\", values=\"mean_acc_synth\")\n",
    "pivot_diff  = summary.pivot(index=\"epsilon\", columns=\"dataset\", values=\"diff_mean\")\n",
    "\n",
    "print(\"\\n=== Mittelwerte synthetische Accuracy (Zeilen: epsilon, Spalten: Datensätze) ===\")\n",
    "print(pivot_synth)\n",
    "\n",
    "print(\"\\n=== Mittelwerte Differenz (synthetic - original) ===\")\n",
    "print(pivot_diff)\n",
    "\n"
   ],
   "id": "641ef2ecce782e87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zusammenfassung pro Dataset & Epsilon ===\n",
      "   dataset  epsilon  mean_acc_synth  std_acc_synth  mean_acc_orig  \\\n",
      "0      acs     0.01        0.890039       0.040162       0.944411   \n",
      "1      acs     0.10        0.918910       0.007714       0.944411   \n",
      "2      acs     0.20        0.923534       0.002783       0.944411   \n",
      "3      acs     0.40        0.924376       0.003211       0.944411   \n",
      "4      acs     0.80        0.924115       0.002299       0.944411   \n",
      "5      acs     1.00        0.922255       0.005471       0.944411   \n",
      "6      acs     2.00        0.923438       0.002430       0.944411   \n",
      "7      acs     4.00        0.923803       0.005056       0.944411   \n",
      "8    adult     0.01        0.786785       0.023646       0.956501   \n",
      "9    adult     0.10        0.821660       0.005295       0.956501   \n",
      "10   adult     0.20        0.817919       0.005274       0.956501   \n",
      "11   adult     0.40        0.818246       0.004068       0.956501   \n",
      "12   adult     0.80        0.816697       0.002554       0.956501   \n",
      "13   adult     1.00        0.817788       0.002838       0.956501   \n",
      "14   adult     2.00        0.817816       0.002426       0.956501   \n",
      "15   adult     4.00        0.818243       0.001836       0.956501   \n",
      "16  br2000     0.01        0.738399       0.072864       0.952263   \n",
      "17  br2000     0.10        0.799651       0.003220       0.952263   \n",
      "18  br2000     0.20        0.801848       0.001413       0.952263   \n",
      "19  br2000     0.40        0.801203       0.001730       0.952263   \n",
      "20  br2000     0.80        0.799595       0.002251       0.952263   \n",
      "21  br2000     1.00        0.800727       0.002072       0.952263   \n",
      "22  br2000     2.00        0.800033       0.001467       0.952263   \n",
      "23  br2000     4.00        0.800579       0.001554       0.952263   \n",
      "\n",
      "    std_acc_orig  n_runs  diff_mean  \n",
      "0            0.0      10  -0.054373  \n",
      "1            0.0      10  -0.025502  \n",
      "2            0.0      10  -0.020877  \n",
      "3            0.0      10  -0.020035  \n",
      "4            0.0      10  -0.020296  \n",
      "5            0.0      10  -0.022156  \n",
      "6            0.0      10  -0.020973  \n",
      "7            0.0      10  -0.020608  \n",
      "8            0.0      10  -0.169717  \n",
      "9            0.0      10  -0.134842  \n",
      "10           0.0      10  -0.138582  \n",
      "11           0.0      10  -0.138256  \n",
      "12           0.0      10  -0.139804  \n",
      "13           0.0      10  -0.138713  \n",
      "14           0.0      10  -0.138686  \n",
      "15           0.0      10  -0.138258  \n",
      "16           0.0      10  -0.213864  \n",
      "17           0.0      10  -0.152612  \n",
      "18           0.0      10  -0.150415  \n",
      "19           0.0      10  -0.151060  \n",
      "20           0.0      10  -0.152668  \n",
      "21           0.0      10  -0.151536  \n",
      "22           0.0      10  -0.152230  \n",
      "23           0.0      10  -0.151684  \n",
      "\n",
      "=== Mittelwerte synthetische Accuracy (Zeilen: epsilon, Spalten: Datensätze) ===\n",
      "dataset       acs     adult    br2000\n",
      "epsilon                              \n",
      "0.01     0.890039  0.786785  0.738399\n",
      "0.10     0.918910  0.821660  0.799651\n",
      "0.20     0.923534  0.817919  0.801848\n",
      "0.40     0.924376  0.818246  0.801203\n",
      "0.80     0.924115  0.816697  0.799595\n",
      "1.00     0.922255  0.817788  0.800727\n",
      "2.00     0.923438  0.817816  0.800033\n",
      "4.00     0.923803  0.818243  0.800579\n",
      "\n",
      "=== Mittelwerte Differenz (synthetic - original) ===\n",
      "dataset       acs     adult    br2000\n",
      "epsilon                              \n",
      "0.01    -0.054373 -0.169717 -0.213864\n",
      "0.10    -0.025502 -0.134842 -0.152612\n",
      "0.20    -0.020877 -0.138582 -0.150415\n",
      "0.40    -0.020035 -0.138256 -0.151060\n",
      "0.80    -0.020296 -0.139804 -0.152668\n",
      "1.00    -0.022156 -0.138713 -0.151536\n",
      "2.00    -0.020973 -0.138686 -0.152230\n",
      "4.00    -0.020608 -0.138258 -0.151684\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
