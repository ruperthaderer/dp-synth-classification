{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-11T18:39:54.353086Z",
     "start_time": "2025-12-11T18:39:49.264610Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "data = pd.read_csv('data\\\\br2000_mrf.csv')\n",
    "\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity/epsilon)\n",
    "\n",
    "data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  10  11  12  13\n",
       "0  0  0  2  1  3  0  0  1  0  0   6   1   5   0\n",
       "1  0  1  1  0  7  0  0  1  0  0   6   1   7   1\n",
       "2  1  1  2  1  6  0  0  1  0  3   4   1   8   1\n",
       "3  0  0  1  0  7  1  0  1  0  0   4   1   6   0\n",
       "4  0  0  6  0  1  1  0  0  0  1   8   1   2   0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:39:54.397009Z",
     "start_time": "2025-12-11T18:39:54.366509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#generate Phase 1\n",
    "\n",
    "def generate(df, numTuples=None, numClass=None, epsilon=1.0):\n",
    "    print(df.shape)\n",
    "    #optionale parameter abklären\n",
    "    if numTuples is None:\n",
    "        numTuples = df.shape[0]\n",
    "    if numClass is None:\n",
    "        numClass = df.shape[1] - 1\n",
    "\n",
    "    className = df.columns[numClass]\n",
    "    attributesName = [col for col in df.columns if col != className]\n",
    "\n",
    "    numHistograms = df.shape[1] - 1\n",
    "    epsilon_per_hist = epsilon/numHistograms\n",
    "\n",
    "    histograms = {} # Dict für jedes Marginal nach Attribut sortiert\n",
    "    for attributeName in attributesName:\n",
    "        counts = df[[attributeName, className]].value_counts() # Series mit MultiIndex, Ebene 0 = Attribut, Ebene 1 = Klasse\n",
    "        noisy_counts = laplace_mech(counts, 1, epsilon_per_hist)\n",
    "        noisy_counts.clip(lower=0.0, inplace=True) # Entfernen von negativen Werten\n",
    "        #noisy_counts = noisy_counts.round()\n",
    "        histograms[attributeName] = noisy_counts\n",
    "\n",
    "    print(histograms[\"1\"])\n",
    "\n",
    "generate(data)\n"
   ],
   "id": "104d19097a9ff58d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38000, 14)\n",
      "1  13\n",
      "0  0     20354.057133\n",
      "1  0      6228.057133\n",
      "   1      5161.057133\n",
      "0  1      3378.057133\n",
      "2  1      1580.057133\n",
      "   0       795.057133\n",
      "3  1       265.057133\n",
      "   0       108.057133\n",
      "4  1        71.057133\n",
      "   0        30.057133\n",
      "5  1        15.057133\n",
      "6  1         6.057133\n",
      "5  0         5.057133\n",
      "6  0         4.057133\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:39:54.420753Z",
     "start_time": "2025-12-11T18:39:54.412451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Hilfsfunktionen ----------\n",
    "\n",
    "def _build_noisy_histograms(df, className, attributesName, epsilon):\n",
    "    \"\"\"\n",
    "    Phase 1: Baue für jedes Attribut ein DP-noisy Histogramm P(attr, class).\n",
    "    \"\"\"\n",
    "    numHistograms = len(attributesName)\n",
    "    epsilon_per_hist = epsilon / numHistograms\n",
    "    histograms = {}\n",
    "\n",
    "    for attributeName in attributesName:\n",
    "        counts = df[[attributeName, className]].value_counts()\n",
    "        noisy_counts = laplace_mech(counts, sensitivity=1, epsilon=epsilon_per_hist)\n",
    "        noisy_counts.clip(lower=0.0, inplace=True)\n",
    "        histograms[attributeName] = noisy_counts\n",
    "\n",
    "    return histograms\n",
    "\n",
    "\n",
    "def _compute_class_distribution(histograms, numTuples):\n",
    "    \"\"\"\n",
    "    Aus allen Histogrammen: classTotals, Klassenwahrscheinlichkeiten p(c),\n",
    "    Anzahl synthetischer Tupel pro Klasse und Klassenvektoren.\n",
    "    \"\"\"\n",
    "    classTotals = {}\n",
    "\n",
    "    # classTotals[c] = Summe über alle Attribute und Attribute-Werte der Counts\n",
    "    for attr_name, hist in histograms.items():\n",
    "        for (attr_val, class_val), count in hist.items():\n",
    "            classTotals[class_val] = classTotals.get(class_val, 0.0) + count\n",
    "\n",
    "    total = sum(classTotals.values())\n",
    "    if total == 0:\n",
    "        # Fallback: gleichverteilt, falls durch Noise alles 0 wurde\n",
    "        gleich = 1.0 / len(classTotals)\n",
    "        p = {c: gleich for c in classTotals}\n",
    "    else:\n",
    "        p = {c: classTotals[c] / total for c in classTotals}\n",
    "\n",
    "    # Anzahl Tupel pro Klasse\n",
    "    classTuples = {c: round(numTuples * p[c]) for c in classTotals}\n",
    "\n",
    "    # Klassenvektor für jede Klasse\n",
    "    class_vector = {c: [c] * classTuples[c] for c in classTuples}\n",
    "\n",
    "    return classTotals, p, classTuples, class_vector\n",
    "\n",
    "\n",
    "def _compute_conditional_attributes(histograms, classTotals):\n",
    "    \"\"\"\n",
    "    Berechne P(attr = a | class = c) für jedes Attribut und jede Klasse.\n",
    "    \"\"\"\n",
    "    cond_attr = {}\n",
    "\n",
    "    for attr_name, hist in histograms.items():\n",
    "        cond_attr[attr_name] = {}\n",
    "        for c in classTotals:\n",
    "            attr_counts = {}\n",
    "\n",
    "            # Zähle für fixe Klasse c die Häufigkeiten der Attribut-Werte\n",
    "            for (attr_val, class_val), count in hist.items():\n",
    "                if class_val == c:\n",
    "                    attr_counts[attr_val] = attr_counts.get(attr_val, 0.0) + count\n",
    "\n",
    "            sum_c = sum(attr_counts.values())\n",
    "\n",
    "            if sum_c == 0:\n",
    "                # gleichverteilte Notlösung\n",
    "                if attr_counts:  # Klasseninfo vorhanden, aber alles 0\n",
    "                    gleich = 1.0 / len(attr_counts)\n",
    "                    probs = {a: gleich for a in attr_counts}\n",
    "                else:\n",
    "                    # gar keine Werte – leeres Dict, kann später geskippt werden\n",
    "                    probs = {}\n",
    "            else:\n",
    "                probs = {a: attr_counts[a] / sum_c for a in attr_counts}\n",
    "\n",
    "            cond_attr[attr_name][c] = probs\n",
    "\n",
    "    return cond_attr\n",
    "\n",
    "\n",
    "def _sample_attribute_vectors(cond_attr, classTuples, random_state=None):\n",
    "    \"\"\"\n",
    "    Ziehe für jedes Attribut und jede Klasse einen Vektor von Attributwerten\n",
    "    mit Länge n_c = classTuples[c] entsprechend P(attr | class).\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    attr_vectors = {}\n",
    "\n",
    "    for attr_name, class_dict in cond_attr.items():\n",
    "        attr_vectors[attr_name] = {}\n",
    "        for c, probs in class_dict.items():\n",
    "            n_c = classTuples[c]\n",
    "\n",
    "            # Falls keine Wahrscheinlichkeiten existieren (leeres Dict)\n",
    "            if not probs:\n",
    "                # Notlösung: Vektor komplett leer, wird später ggf. ersetzt/geskippt\n",
    "                attr_vectors[attr_name][c] = [None] * n_c\n",
    "                continue\n",
    "\n",
    "            # Zielanzahl pro Attributwert\n",
    "            target_count = {attr_val: round(p_val * n_c)\n",
    "                            for attr_val, p_val in probs.items()}\n",
    "\n",
    "            # Baue den Vektor mit der Zielanzahl pro Wert\n",
    "            attr_vec_c = []\n",
    "            for attr_val, cnt in target_count.items():\n",
    "                attr_vec_c.extend([attr_val] * cnt)\n",
    "\n",
    "            # Länge anpassen (auf n_c)\n",
    "            current_len = len(attr_vec_c)\n",
    "            diff = n_c - current_len\n",
    "\n",
    "            if diff > 0:\n",
    "                vals = list(probs.keys())\n",
    "                extra = np.random.choice(vals, size=diff, p=list(probs.values()))\n",
    "                attr_vec_c.extend(extra)\n",
    "            elif diff < 0:\n",
    "                remove_indices = np.random.choice(len(attr_vec_c), size=-diff, replace=False)\n",
    "                for idx in sorted(remove_indices, reverse=True):\n",
    "                    attr_vec_c.pop(idx)\n",
    "\n",
    "            # Shuffle, um keine Struktur / Reihenfolge zu verraten\n",
    "            np.random.shuffle(attr_vec_c)\n",
    "\n",
    "            attr_vectors[attr_name][c] = attr_vec_c\n",
    "\n",
    "    return attr_vectors\n",
    "\n",
    "\n",
    "def _assemble_synthetic_dataframe(attr_vectors, class_vector, attributesName, className):\n",
    "    \"\"\"\n",
    "    Setze aus Attributvektoren und Klassenvektoren die DataFrames pro Klasse\n",
    "    und führe sie zu einem Gesamt-DataFrame zusammen.\n",
    "    \"\"\"\n",
    "    blocks = {}\n",
    "    for c, class_vec in class_vector.items():\n",
    "        n_c = len(class_vec)\n",
    "        block = {}\n",
    "\n",
    "        for attributeName in attributesName:\n",
    "            values = attr_vectors[attributeName][c]\n",
    "            # Sanity-Check: auf Länge n_c trimmen/auffüllen falls nötig\n",
    "            if len(values) < n_c:\n",
    "                values = values + [values[-1]] * (n_c - len(values))\n",
    "            elif len(values) > n_c:\n",
    "                values = values[:n_c]\n",
    "            block[attributeName] = values\n",
    "\n",
    "        block[className] = class_vec\n",
    "        blocks[c] = block\n",
    "\n",
    "    df_blocks = {c: pd.DataFrame(block) for c, block in blocks.items()}\n",
    "    synthetic_df = pd.concat(df_blocks.values(), ignore_index=True)\n",
    "\n",
    "    return synthetic_df\n",
    "\n",
    "\n",
    "# ---------- Hauptfunktion ----------\n",
    "\n",
    "def generate(df, numTuples=None, numClass=None, epsilon=1.0, random_state=42):\n",
    "    \"\"\"\n",
    "    Erzeuge differentially private synthetische Daten auf Basis von df.\n",
    "    Phase 1: DP-Histogramme.\n",
    "    Phase 2: Ziehen von Klassen- und Attributwerten und Zusammenbau des synthetischen Datensatzes.\n",
    "    \"\"\"\n",
    "    print(df.shape)\n",
    "\n",
    "    # optionale Parameter abklären\n",
    "    if numTuples is None:\n",
    "        numTuples = df.shape[0]\n",
    "    if numClass is None:\n",
    "        numClass = df.shape[1] - 1\n",
    "\n",
    "    className = df.columns[numClass]\n",
    "    attributesName = [col for col in df.columns if col != className]\n",
    "\n",
    "    # ---- Phase 1: DP-Histogramme ----\n",
    "    histograms = _build_noisy_histograms(df, className, attributesName, epsilon)\n",
    "\n",
    "    # ---- Phase 2: Klassenverteilung, P(attr|class), Sampling, Zusammenbau ----\n",
    "    classTotals, p, classTuples, class_vector = _compute_class_distribution(\n",
    "        histograms, numTuples\n",
    "    )\n",
    "\n",
    "    cond_attr = _compute_conditional_attributes(histograms, classTotals)\n",
    "\n",
    "    attr_vectors = _sample_attribute_vectors(\n",
    "        cond_attr, classTuples, random_state=random_state\n",
    "    )\n",
    "\n",
    "    synthetic_df = _assemble_synthetic_dataframe(\n",
    "        attr_vectors, class_vector, attributesName, className\n",
    "    )\n",
    "\n",
    "    # final shuffle des gesamten DataFrames (Zeilen)\n",
    "    synthetic_df = synthetic_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return synthetic_df\n",
    "\n",
    "# Beispiel-Aufruf:\n",
    "# synthetic_data = generate(data, epsilon=1.0)\n"
   ],
   "id": "982ce399b08d509a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:39:54.463902Z",
     "start_time": "2025-12-11T18:39:54.460340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#GPT Zelle\n",
    "def build_pipeline(clf=None):\n",
    "    \"\"\"\n",
    "    Baut eine sklearn-Pipeline:\n",
    "    - OneHotEncoding für alle Feature-Spalten (alle außer der letzten)\n",
    "    - Classifier (default: RandomForest, kann aber übergeben werden)\n",
    "    \"\"\"\n",
    "    if clf is None:\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # Alle Spalten außer der letzten sind Features\n",
    "    # (funktioniert auch mit '0', '1', ..., '13' als Spaltennamen)\n",
    "    def make_preprocessor(df):\n",
    "        feature_cols = df.columns[:-1]\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), feature_cols)\n",
    "            ]\n",
    "        )\n",
    "        return preprocessor\n",
    "\n",
    "    # kleine Wrapperfunktion, damit wir df nicht global brauchen\n",
    "    def make_model(df):\n",
    "        preprocessor = make_preprocessor(df)\n",
    "        model = Pipeline(steps=[\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", clf)\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    return make_model\n",
    "\n",
    "\n",
    "def evaluate_df(df, clf=None, n_splits=10, test_size=0.2, base_random_state=0):\n",
    "    \"\"\"\n",
    "    - df: DataFrame mit letzter Spalte = Klasse\n",
    "    - clf: optionaler Classifier (sonst RandomForest)\n",
    "    - n_splits: wie viele verschiedene Train/Test-Splits (mit unterschiedlichen seeds)\n",
    "    - test_size: Anteil Testdaten\n",
    "    \"\"\"\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    make_model = build_pipeline(clf)\n",
    "\n",
    "    accuracies = []\n",
    "    confusion_matrices = []\n",
    "\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        rs = base_random_state + i\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=test_size,\n",
    "            random_state=rs,\n",
    "            stratify=y  # damit Klassenverteilung im Split erhalten bleibt\n",
    "        )\n",
    "\n",
    "        model = make_model(df)   # baut Pipeline mit passenden Spalten\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = (y_pred == y_test).mean()\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "        confusion_matrices.append(cm)\n",
    "\n",
    "    return {\n",
    "        \"classes\": classes,\n",
    "        \"accuracies\": np.array(accuracies),\n",
    "        \"confusion_matrices\": confusion_matrices,\n",
    "    }\n"
   ],
   "id": "a2b3a07b1420e0d1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:41:14.306284Z",
     "start_time": "2025-12-11T18:39:54.542509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Synthetische Daten\n",
    "synthetic_data = generate(data)\n",
    "synth_results = evaluate_df(synthetic_data, n_splits=10)\n",
    "\n",
    "# Originaldaten (data) aus CSV\n",
    "orig_results = evaluate_df(data, n_splits=10)\n",
    "\n",
    "print(\"Originaldaten:\")\n",
    "print(\"  Accuracy mean:\", orig_results[\"accuracies\"].mean())\n",
    "print(\"  Accuracy std :\", orig_results[\"accuracies\"].std())\n",
    "print(\"  Confusion Matrix (Split 0):\")\n",
    "print(orig_results[\"confusion_matrices\"][0])\n",
    "print(\"  Klassenreihenfolge:\", orig_results[\"classes\"])\n",
    "\n",
    "print(\"\\nSynthetische Daten:\")\n",
    "print(\"  Accuracy mean:\", synth_results[\"accuracies\"].mean())\n",
    "print(\"  Accuracy std :\", synth_results[\"accuracies\"].std())\n",
    "print(\"  Confusion Matrix (Split 0):\")\n",
    "print(synth_results[\"confusion_matrices\"][0])\n",
    "print(\"  Klassenreihenfolge:\", synth_results[\"classes\"])\n",
    "\n",
    "synthetic_data.head()"
   ],
   "id": "f436ed03d7c446f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38000, 14)\n",
      "Originaldaten:\n",
      "  Accuracy mean: 0.8010131578947368\n",
      "  Accuracy std : 0.003641695481967089\n",
      "  Confusion Matrix (Split 0):\n",
      "[[4964  541]\n",
      " [ 938 1157]]\n",
      "  Klassenreihenfolge: [0 1]\n",
      "\n",
      "Synthetische Daten:\n",
      "  Accuracy mean: 0.8196578947368423\n",
      "  Accuracy std : 0.003070162907242606\n",
      "  Confusion Matrix (Split 0):\n",
      "[[5056  450]\n",
      " [ 893 1201]]\n",
      "  Klassenreihenfolge: [0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  10  11  12  13\n",
       "0  0  0  3  2  4  0  0  0  0  1   7   1   6   1\n",
       "1  0  0  4  2  2  0  0  0  0  0   2   1   5   0\n",
       "2  0  0  6  2  9  0  0  1  0  2   5   1   1   0\n",
       "3  1  1  2  0  3  1  0  1  0  1   7   1   5   0\n",
       "4  0  0  2  6  1  1  0  1  0  0   5   1   3   0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:41:33.897812Z",
     "start_time": "2025-12-11T18:41:14.595647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "#version mit alternativem clf\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Synthetische Daten\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "synthetic_data = generate(data)   # deine Funktion\n",
    "synth_results = evaluate_df(synthetic_data, clf=clf, n_splits=10)\n",
    "\n",
    "# Originaldaten (data) aus CSV\n",
    "orig_results = evaluate_df(data, clf=clf, n_splits=10)\n",
    "\n",
    "print(\"Originaldaten:\")\n",
    "print(\"  Accuracy mean:\", orig_results[\"accuracies\"].mean())\n",
    "print(\"  Accuracy std :\", orig_results[\"accuracies\"].std())\n",
    "print(\"  Confusion Matrix (Split 0):\")\n",
    "print(orig_results[\"confusion_matrices\"][0])\n",
    "print(\"  Klassenreihenfolge:\", orig_results[\"classes\"])\n",
    "\n",
    "print(\"\\nSynthetische Daten:\")\n",
    "print(\"  Accuracy mean:\", synth_results[\"accuracies\"].mean())\n",
    "print(\"  Accuracy std :\", synth_results[\"accuracies\"].std())\n",
    "print(\"  Confusion Matrix (Split 0):\")\n",
    "print(synth_results[\"confusion_matrices\"][0])\n",
    "print(\"  Klassenreihenfolge:\", synth_results[\"classes\"])\n",
    "'''"
   ],
   "id": "7b03d8a215a8cd20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38000, 14)\n",
      "Originaldaten:\n",
      "  Accuracy mean: 0.7616315789473684\n",
      "  Accuracy std : 0.004743124488867803\n",
      "  Confusion Matrix (Split 0):\n",
      "[[4716  789]\n",
      " [1015 1080]]\n",
      "  Klassenreihenfolge: [0 1]\n",
      "\n",
      "Synthetische Daten:\n",
      "  Accuracy mean: 0.7530921052631581\n",
      "  Accuracy std : 0.0032727204961596275\n",
      "  Confusion Matrix (Split 0):\n",
      "[[4550  957]\n",
      " [ 926 1167]]\n",
      "  Klassenreihenfolge: [0 1]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T22:33:28.559810Z",
     "start_time": "2025-12-11T18:47:41.493885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_epsilon_study(datasets,          # dict: {\"adult\": df_adult, \"bank\": df_bank, ...}\n",
    "                      epsilons,          # Liste der Epsilon-Werte\n",
    "                      n_reps=10,\n",
    "                      n_splits=10):\n",
    "    \"\"\"\n",
    "    Führt für alle Datensätze und alle Epsilon-Werte eine Experimental-Studie durch.\n",
    "    Gibt ein DataFrame mit Ergebnissen zurück.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for ds_name, df in datasets.items():\n",
    "        print(f\"Dataset: {ds_name}\")\n",
    "        for eps in epsilons:\n",
    "            for rep in range(n_reps):\n",
    "                # synthetische Daten generieren\n",
    "                synth = generate(df, epsilon=eps, random_state=42 + rep)\n",
    "\n",
    "                # Klassifikations-Performance auf synthetischen Daten messen\n",
    "                acc_synth = evaluate_df(synth, n_splits=n_splits)\n",
    "\n",
    "                # Optional: Performance auf Originaldaten (Baseline)\n",
    "                acc_orig = evaluate_df(df, n_splits=n_splits)\n",
    "\n",
    "                results.append({\n",
    "                    \"dataset\": ds_name,\n",
    "                    \"epsilon\": eps,\n",
    "                    \"rep\": rep,\n",
    "                    \"accuracy_synth\": acc_synth,\n",
    "                    \"accuracy_orig\": acc_orig\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "acs = pd.read_csv('data\\\\acs_mrf.csv')\n",
    "adult = pd.read_csv('data\\\\adult_mrf.csv')\n",
    "br2000 = pd.read_csv('data\\\\br2000_mrf.csv')\n",
    "# Beispiel-Setup:\n",
    "datasets = {\n",
    "    \"acs\": acs,\n",
    "    \"adult\": adult,\n",
    "    \"br2000\": br2000,\n",
    " }\n",
    "epsilons = [0.01, 0.1, 0.2, 0.4, 0.8, 1.0, 2.0, 4.0]\n",
    "study_results = run_epsilon_study(datasets, epsilons, n_reps=10, n_splits=10)\n"
   ],
   "id": "7e845ffcf19846b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: acs\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "(47461, 23)\n",
      "Dataset: adult\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "(45222, 15)\n",
      "Dataset: br2000\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n",
      "(38000, 14)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T12:38:01.960379Z",
     "start_time": "2025-12-12T12:38:01.937369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import ast\n",
    "\n",
    "# Dict-Spalten -> mean-Accuracy-Spalten\n",
    "\n",
    "def extract_mean_accuracy(x):\n",
    "    # x kann schon ein dict oder ein String sein\n",
    "    if isinstance(x, dict):\n",
    "        d = x\n",
    "    else:\n",
    "        try:\n",
    "            d = ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    accs = d.get(\"accuracies\", None)\n",
    "    if accs is None or len(accs) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return float(np.mean(accs))\n",
    "\n",
    "study_results[\"acc_synth_mean\"] = study_results[\"accuracy_synth\"].apply(extract_mean_accuracy)\n",
    "study_results[\"acc_orig_mean\"]  = study_results[\"accuracy_orig\"].apply(extract_mean_accuracy)\n",
    "\n",
    "# Zusammenfassung pro Datensatz & Epsilon\n",
    "\n",
    "summary = (\n",
    "    study_results\n",
    "    .groupby([\"dataset\", \"epsilon\"])\n",
    "    .agg(\n",
    "        mean_acc_synth=(\"acc_synth_mean\", \"mean\"),\n",
    "        std_acc_synth=(\"acc_synth_mean\", \"std\"),\n",
    "        mean_acc_orig=(\"acc_orig_mean\", \"mean\"),\n",
    "        std_acc_orig=(\"acc_orig_mean\", \"std\"),\n",
    "        n_runs=(\"rep\", \"nunique\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary[\"diff_mean\"] = summary[\"mean_acc_synth\"] - summary[\"mean_acc_orig\"]\n",
    "summary = summary.sort_values([\"dataset\", \"epsilon\"])\n",
    "\n",
    "# Kurz ausgeben & (optional) Pivot-Tabellen für weitere Auswertung \n",
    "\n",
    "print(\"=== Zusammenfassung pro Dataset & Epsilon ===\")\n",
    "print(summary)\n",
    "\n",
    "pivot_synth = summary.pivot(index=\"epsilon\", columns=\"dataset\", values=\"mean_acc_synth\")\n",
    "pivot_diff  = summary.pivot(index=\"epsilon\", columns=\"dataset\", values=\"diff_mean\")\n",
    "\n",
    "print(\"\\n=== Mittelwerte synthetische Accuracy (Zeilen: epsilon, Spalten: Datensätze) ===\")\n",
    "print(pivot_synth)\n",
    "\n",
    "print(\"\\n=== Mittelwerte Differenz (synthetic - original) ===\")\n",
    "print(pivot_diff)\n",
    "\n"
   ],
   "id": "641ef2ecce782e87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zusammenfassung pro Dataset & Epsilon ===\n",
      "   dataset  epsilon  mean_acc_synth  std_acc_synth  mean_acc_orig  \\\n",
      "0      acs     0.01        0.960541       0.035032       0.935626   \n",
      "1      acs     0.10        0.939149       0.004430       0.935626   \n",
      "2      acs     0.20        0.938933       0.003146       0.935626   \n",
      "3      acs     0.40        0.937477       0.001863       0.935626   \n",
      "4      acs     0.80        0.937901       0.000850       0.935626   \n",
      "5      acs     1.00        0.937059       0.001245       0.935626   \n",
      "6      acs     2.00        0.937372       0.000966       0.935626   \n",
      "7      acs     4.00        0.937624       0.000852       0.935626   \n",
      "8    adult     0.01        0.941132       0.039721       0.838132   \n",
      "9    adult     0.10        0.917034       0.008488       0.838132   \n",
      "10   adult     0.20        0.915504       0.003143       0.838132   \n",
      "11   adult     0.40        0.915158       0.004970       0.838132   \n",
      "12   adult     0.80        0.914519       0.002295       0.838132   \n",
      "13   adult     1.00        0.914450       0.001666       0.838132   \n",
      "14   adult     2.00        0.914837       0.001653       0.838132   \n",
      "15   adult     4.00        0.914052       0.001213       0.838132   \n",
      "16  br2000     0.01        0.869947       0.059093       0.801013   \n",
      "17  br2000     0.10        0.817396       0.004592       0.801013   \n",
      "18  br2000     0.20        0.819150       0.003833       0.801013   \n",
      "19  br2000     0.40        0.818722       0.002518       0.801013   \n",
      "20  br2000     0.80        0.819091       0.001135       0.801013   \n",
      "21  br2000     1.00        0.819428       0.001833       0.801013   \n",
      "22  br2000     2.00        0.818625       0.001841       0.801013   \n",
      "23  br2000     4.00        0.818337       0.001901       0.801013   \n",
      "\n",
      "    std_acc_orig  n_runs  diff_mean  \n",
      "0            0.0      10   0.024915  \n",
      "1            0.0      10   0.003523  \n",
      "2            0.0      10   0.003307  \n",
      "3            0.0      10   0.001851  \n",
      "4            0.0      10   0.002274  \n",
      "5            0.0      10   0.001433  \n",
      "6            0.0      10   0.001745  \n",
      "7            0.0      10   0.001997  \n",
      "8            0.0      10   0.103001  \n",
      "9            0.0      10   0.078902  \n",
      "10           0.0      10   0.077372  \n",
      "11           0.0      10   0.077026  \n",
      "12           0.0      10   0.076387  \n",
      "13           0.0      10   0.076318  \n",
      "14           0.0      10   0.076705  \n",
      "15           0.0      10   0.075920  \n",
      "16           0.0      10   0.068934  \n",
      "17           0.0      10   0.016383  \n",
      "18           0.0      10   0.018137  \n",
      "19           0.0      10   0.017709  \n",
      "20           0.0      10   0.018078  \n",
      "21           0.0      10   0.018414  \n",
      "22           0.0      10   0.017612  \n",
      "23           0.0      10   0.017324  \n",
      "\n",
      "=== Mittelwerte synthetische Accuracy (Zeilen: epsilon, Spalten: Datensätze) ===\n",
      "dataset       acs     adult    br2000\n",
      "epsilon                              \n",
      "0.01     0.960541  0.941132  0.869947\n",
      "0.10     0.939149  0.917034  0.817396\n",
      "0.20     0.938933  0.915504  0.819150\n",
      "0.40     0.937477  0.915158  0.818722\n",
      "0.80     0.937901  0.914519  0.819091\n",
      "1.00     0.937059  0.914450  0.819428\n",
      "2.00     0.937372  0.914837  0.818625\n",
      "4.00     0.937624  0.914052  0.818337\n",
      "\n",
      "=== Mittelwerte Differenz (synthetic - original) ===\n",
      "dataset       acs     adult    br2000\n",
      "epsilon                              \n",
      "0.01     0.024915  0.103001  0.068934\n",
      "0.10     0.003523  0.078902  0.016383\n",
      "0.20     0.003307  0.077372  0.018137\n",
      "0.40     0.001851  0.077026  0.017709\n",
      "0.80     0.002274  0.076387  0.018078\n",
      "1.00     0.001433  0.076318  0.018414\n",
      "2.00     0.001745  0.076705  0.017612\n",
      "4.00     0.001997  0.075920  0.017324\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
